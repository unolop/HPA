{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import sys \n",
    "import numpy as np \n",
    "sys.path.append('/home/work/yuna/VLMEval') \n",
    "from dataset.dataloader import PostProcessor # contractions, numbers, punctuations\n",
    "from analysis.pilot.analysis import vqa_score, str_to_list \n",
    "from analysis.utils.question_type_mapper import question_type \n",
    "from typing import List, Any\n",
    "import pandas as pd \n",
    "from glob import glob \n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np \n",
    "from utils import answer_similarity, contains_korean\n",
    "    \n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "postprocessor = PostProcessor()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [] \n",
    "for f in glob('/home/work/yuna/VLMEval/analysis/pilot/questions/blind*.csv'): # Question paths \n",
    "    df = pd.read_csv(f) \n",
    "    # print(f, len(df.columns))\n",
    "    qs.append(df) \n",
    "    \n",
    "qs = pd.concat(qs)\n",
    "qs = qs[['question_id', 'question', 'mean_acc']] # drop everything except question_id and question and mean_acc  \n",
    "qs = pd.merge(qs, adf, on=['question_id'], how='left')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vqa(row):\n",
    "    qid = int(row.get(\"question_id\", \"\").strip()) \n",
    "    ans = get_answer(adf, qid) \n",
    "    processed_output = postprocessor.postprocess_answer(row.get(\"output\", \"\")) \n",
    "    acc = vqa_score(processed_output, ans) \n",
    "    score = answer_similarity(processed_output, ans)\n",
    "    item = {\n",
    "            \"qid\": row.get(\"question_id\", \"\").strip(),\n",
    "            \"question\": row.get(\"question\", \"\").strip(),  \n",
    "            \"question_type\": question_type[int(row.get(\"question_id\", \"\").strip())],\n",
    "            \"model\": row.get(\"model\", \"\").strip(),\n",
    "            \"answer\": ans, \n",
    "            # \"filename\": row.get(\"filename\", \"\").strip(),\n",
    "            \"raw_output\": row.get(\"output\", \"\").strip(),\n",
    "            \"processed_output\": processed_output, # row.get(\"processed_output\", \"\").strip(),\n",
    "            \"acc\": acc,\n",
    "            'score': float(score) \n",
    "            }\n",
    "    # print(item) \n",
    "    return item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob('/home/work/yuna/VLMEval/lm_vqa/*'):  \n",
    "    convert_results(file, \"LLM\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_human_results(folder='./responses'): \n",
    "    dfs=[]\n",
    "    for f in glob(f'{folder}/*.xlsx'): \n",
    "        print(f) \n",
    "        dfs.append(pd.read_excel(f)) \n",
    "    df = pd.concat(dfs)\n",
    "    df = df.melt(id_vars=['Timestamp', \"이름 또는 이니셜 Name or Initials \", 'Score']).rename(columns={'variable': 'question', 'value': 'answer', \"이름 또는 이니셜 Name or Initials \": \"subj\"})\n",
    "    # df['question'] = df['question'].str.split('\\n').str[0]\n",
    "    df[['question', 'question (Korean)']] = df['question'].str.split('\\n', expand=True)[[0,1]] \n",
    "    df['answer'] = df['answer'].replace({'Yes 네': 'yes', \"No 아니오\": \"no\"}) \n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/yuna/VLMEval/analysis/pilot/responses/VQA-SPUB Pilot Experiment (Responses) (1).xlsx\n",
      "/home/work/yuna/VLMEval/analysis/pilot/responses/VQA-SPUB Pilot Experiment Pt.2 (Responses) (1).xlsx\n",
      "/home/work/yuna/VLMEval/analysis/pilot/responses/VQA-SPUB Pilot Experiment Pt.3 (Responses).xlsx\n"
     ]
    }
   ],
   "source": [
    "hm = read_human_results(\"/home/work/yuna/VLMEval/analysis/pilot/responses\") \n",
    "# hm = pd.merge(hm, adf, on=['question'], how='left') \n",
    "hm['question'] = hm['question'].str.replace('scene', 'photo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81573"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['성별', '연락처 (필수 아님)', 'Name or Initials 이름 또는 이니셜 '], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm = pd.merge(hm, allqs, on=['question'], how='left') \n",
    "hm['answer'] = hm['answer'].replace({'Yes 네': 'yes', \"No 아니오\": \"no\"}) \n",
    "mask = hm['question_id'].isna()  \n",
    "hm[mask].question.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10255"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm['subj'] = hm['subj'].fillna(hm['Timestamp']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "    \n",
    "korean_translations = {} \n",
    "kr = pd.concat([pd.read_csv('/home/work/yuna/VLMEval/analysis/pilot/responses/korean_answers_translated.csv'),\n",
    "                pd.read_csv('/home/work/yuna/VLMEval/analysis/pilot/responses/korean_answers_translated (2).csv')]) \n",
    "\n",
    "for i, r in kr.iterrows(): \n",
    "    kr_ans = r['korean'] \n",
    "    kr_ans = postprocessor.postprocess_answer(kr_ans)  \n",
    "    en_ans = r['answer'] \n",
    "    if kr_ans not in korean_translations.keys(): \n",
    "        korean_translations[kr_ans] = en_ans \n",
    "len(korean_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(need_tran).to_csv('/home/work/yuna/hpa/eda/need_translate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./translate.csv')\n",
    "tr = tr.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col = tr.columns[0]  # e.g., 'question'\n",
    "second_col = tr.columns[1]  # e.g., 'subj' or 'answer'\n",
    "dictionary = dict(zip(tr[first_col].astype(str), tr[second_col].astype(str))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_translations.update(dictionary)\n",
    "korean_translations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10150"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = hm.dropna(subset=['question_id'], axis=0)  \n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save dictionary to Python file\n",
    "# def save_dict_to_python_file(dictionary, filename):\n",
    "with open('./korean_translations.py', 'w', encoding='utf-8') as f:\n",
    "    f.write('korean_translations = ')\n",
    "    json.dump(korean_translations, f, ensure_ascii=False, indent=4)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/성민석.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/Kathy.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/Winnie.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/안덕현.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/성희.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/박지윤.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/박정원.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/Cm.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/박호조.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/Dowon Hwang.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/조희승.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/EJ.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/BJH.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/cm.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/Park.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/kathy.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/HJp.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/이성희 .jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/주연.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-29 08:23:54.109000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-29 09:07:33.096000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-29 18:22:09.402000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-29 19:38:19.667000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-30 00:22:45.913000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-30 06:24:35.181000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-30 11:10:46.878000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-30 11:18:51.944000.jsonl\n",
      "✅ Saved JSONL to: /home/work/yuna/hpa/results/humans_processed/2025-09-30 17:23:56.895000.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_folder = f\"/home/work/yuna/hpa/results/humans_processed\" \n",
    "os.makedirs(output_folder, exist_ok=True) \n",
    "need_tran=[] \n",
    "\n",
    "for subj in df.subj.unique(): \n",
    "    tmp = df[df['subj'] == subj] \n",
    "    filename = os.path.join(output_folder, f'{subj}.jsonl' ) \n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as fout: \n",
    "        for i, row in tmp.iterrows():  \n",
    "            # print(row)\n",
    "            qid = row['question_id'] \n",
    "            # print(int(qid) )\n",
    "            output = row.get(\"answer\", \"\") \n",
    "            processed_output = postprocessor.postprocess_answer(str(output)) \n",
    "            ans = get_answer(adf, qid) \n",
    "            \n",
    "            if contains_korean(processed_output): \n",
    "                if processed_output in korean_translations.keys(): \n",
    "                    processed_output = korean_translations[processed_output] \n",
    "                else: \n",
    "                    print(row.get(\"question\", \"\").strip(), processed_output) \n",
    "                    if processed_output not in need_tran:\n",
    "                        need_tran.append(processed_output)\n",
    "            acc = vqa_score(processed_output, ans) \n",
    "            score = float(answer_similarity(processed_output, ans))\n",
    "            \n",
    "            item = {\n",
    "                    \"qid\": qid,\n",
    "                    \"question\": row.get(\"question\", \"\").strip(),  \n",
    "                    \"question_type\": question_type[qid],\n",
    "                    \"subj\": str(subj), \n",
    "                    # \"answer\": ans, \n",
    "                    \"raw_output\": output, \n",
    "                    \"processed_output\": processed_output, \n",
    "                    \"acc\": acc ,\n",
    "                    'score': score \n",
    "                    }\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "    print(f\"✅ Saved JSONL to: {filename}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacyovod_new_copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
