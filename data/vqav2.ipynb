{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "from dataset.vqav2 import VQADataset\n",
    "dataset = VQADataset() # \"/home/work/yuna/HPA/data/vqav2_val.json\" \n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 'COCO_val2014_000000189936.jpg',\n",
       " 'question_id': 189936002,\n",
       " 'question_type': 'what is the woman',\n",
       " 'question': 'Question: What is the woman in gray doing? Answer the question using a single word or phrase. \\nAnswer:',\n",
       " 'answers': [{'answer': 'taking pictures',\n",
       "   'answer_confidence': 'yes',\n",
       "   'answer_id': 1},\n",
       "  {'answer': 'taking photo', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "  {'answer': 'photographing', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "  {'answer': 'taking pictures', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "  {'answer': 'taking picture', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "  {'answer': 'taking picture', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "  {'answer': 'photographing', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "  {'answer': 'taking picture', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "  {'answer': 'photographing', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "  {'answer': 'taking picture', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       " 'multiple_choice_answer': 'taking picture',\n",
       " 'answer_type': 'other'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/work/yuna/HPA/data/swift/vlm_vqav2_val_1k_blind.jsonl\n",
      "saved to /home/work/yuna/HPA/data/swift/vlm_vqav2_val_1k.jsonl\n"
     ]
    }
   ],
   "source": [
    "# swift_dataset(dataset)\n",
    "swift_dataset(dataset, 'vlm')\n",
    "swift_dataset(dataset, 'vlm', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder=\"/home/work/yuna/VLMEval/data/val2014\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swift_dataset(vqa_data, models='llm', blind=True): \n",
    "\n",
    "    output_path=    f\"/home/work/yuna/HPA/data/swift/{models}_vqav2_val_1k\"\n",
    "    if blind: \n",
    "        output_path += \"_blind\"\n",
    "    output_path += \".jsonl\"\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in vqa_data:\n",
    "            question = item['question']\n",
    "            if not models == 'llm':\n",
    "                question = f\"<image>\\n{question}\"\n",
    "            \n",
    "            swift_item = {\n",
    "                \"id\": str(item.get('question_id', item.get('image_id', ''))),\n",
    "                \"conversation\": [\n",
    "                    {\n",
    "                        \"from\": \"human\",\n",
    "                        \"value\": question\n",
    "                    },\n",
    "                    # {\n",
    "                    #     \"from\": \"assistant\",\n",
    "                    #     \"value\": item['multiple_choice_answer']\n",
    "                    # }\n",
    "                ]\n",
    "            }\n",
    "            if not models == 'llm':\n",
    "                image_path = os.path.join(image_folder, item['image_id'])\n",
    "                if blind: \n",
    "                    swift_item['image'] = [\"/home/work/yuna/HPA/data/blank_224.png\"]\n",
    "                else: \n",
    "                   swift_item['image'] = [image_path]\n",
    "            \n",
    "            f.write(json.dumps(swift_item, ensure_ascii=False) + '\\n')\n",
    "    print(f\"saved to {output_path}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TSV with full VQA annotations to: /home/work/yuna/HPA/data/vqav2_1k/val.tsv\n"
     ]
    }
   ],
   "source": [
    "modelscope_tsv(dataset, '/home/work/yuna/HPA/data/vqav2_1k/val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os \n",
    "import json \n",
    "\n",
    "def modelscope_tsv(items, output_path, image_folder='/home/work/yuna/VLMEval/data/val2014'):\n",
    "    \"\"\"\n",
    "    Write a list of items to a TSV file with columns:\n",
    "    index, answer, question, image_path\n",
    "\n",
    "    index\tanswer\tquestion\t        image_path\n",
    "    1\tDog\tWhat animal is this?\t    /root/LMUData/images/custom_mcq/dog.jpg\n",
    "    2\tMuseum\tWhat building is this?\t/root/LMUData/images/custom_mcq/AMNH.jpg\n",
    "    3\tTokyo\tWhich city's skyline is this?\t/root/LMUData/images/custom_mcq/tokyo.jpg\n",
    "    4\tTesla\tWhat is the brand of this car?\t/root/LMUData/images/custom_mcq/tesla.jpg\n",
    "    5\tRunning\tWhat is the person in the picture doing?\t/root/LMUData/images/custom_mcq/running.jpg\n",
    "    \n",
    "    Parameters:\n",
    "        data: list of dicts, each containing:\n",
    "              {\n",
    "                  \"answer\": str,\n",
    "                  \"question\": str,\n",
    "                  \"image_path\": str\n",
    "              }\n",
    "        output_path: str, path to write .tsv file\n",
    "    \"\"\"\n",
    "    fieldnames = [\n",
    "        \"index\",\n",
    "        \"image_id\",\n",
    "        \"question_id\",\n",
    "        \"question_type\",\n",
    "        \"answer_type\",\n",
    "        \"question\",\n",
    "        \"answer\",\n",
    "        \"image_path\",\n",
    "    ]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=\"\\t\")\n",
    "        writer.writeheader()\n",
    "\n",
    "        for idx, item in enumerate(items, start=1):\n",
    "            writer.writerow({\n",
    "                \"index\": idx,\n",
    "                \"image_id\": item[\"image_id\"],\n",
    "                \"question_id\": item[\"question_id\"],\n",
    "                \"question_type\": item[\"question_type\"],\n",
    "                \"answer_type\": item[\"answer_type\"],\n",
    "                \"question\": item[\"question\"],\n",
    "                \"answers\": json.dumps(item[\"answers\"], ensure_ascii=False),\n",
    "                \"multiple_choice_answer\": json.dumps(item[\"multiple_choice_answer\"], ensure_ascii=False),\n",
    "                \"image_path\": os.path.join(image_folder, item[\"image_id\"]),\n",
    "            })\n",
    "\n",
    "    print(f\"Saved TSV with full VQA annotations to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacyovod_new_copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
