{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating val split: 100%|██████████| 1500/1500 [00:00<00:00, 3166.54 examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLin-Chen/MMStar\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# blank image \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)) \n\u001b[1;32m      6\u001b[0m image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./blank.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\"Lin-Chen/MMStar\", split=\"val\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'image', 'answer', 'category', 'l2_category', 'meta_info'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "\n",
    "# blank image \n",
    "image = Image.new('RGB', (224, 224), color=(0, 0, 0)) \n",
    "image.save('./blank_224.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'question': 'Which option describe the object relationship in the image correctly?\\nOptions: A: The suitcase is on the book., B: The suitcase is beneath the cat., C: The suitcase is beneath the bed., D: The suitcase is beneath the book.',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x384>,\n",
       " 'answer': 'A',\n",
       " 'category': 'coarse perception',\n",
       " 'l2_category': 'image scene and topic',\n",
       " 'meta_info': {'source': 'MMBench',\n",
       "  'split': 'val',\n",
       "  'image_path': 'images/0.jpg'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/work/yuna/HPA/data/swift/vlm_mmstar_blind.jsonl\n"
     ]
    }
   ],
   "source": [
    "# swift_dataset(dataset)\n",
    "swift_dataset(dataset, 'vlm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swift_dataset(vqa_data, models='llm', blind=True): \n",
    "\n",
    "    output_path=f\"/home/work/yuna/HPA/data/swift/{models}_mmstar\"\n",
    "    if blind: \n",
    "        output_path += \"_blind\"\n",
    "    output_path += \".jsonl\"\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in vqa_data:\n",
    "            question = item['question']\n",
    "            if not models == 'llm':\n",
    "                question = f\"<image>\\n{question}\"\n",
    "            \n",
    "            item['conversation'] = [\n",
    "                    {\n",
    "                        \"from\": \"human\",\n",
    "                        \"value\": question\n",
    "                    },\n",
    "                    # {\n",
    "                    #     \"from\": \"assistant\",\n",
    "                    #     \"value\": item['multiple_choice_answer']\n",
    "                    # }\n",
    "                ]\n",
    "            if not models == 'llm':\n",
    "                if blind: \n",
    "                    item['image'] = [\"/home/work/yuna/HPA/data/blank_224.png\"]\n",
    "                else: \n",
    "                   item['image'] = [os.path.join(image_folder, item['image_id'])]\n",
    "            else: \n",
    "               item.pop('image')\n",
    " \n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    print(f\"saved to {output_path}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TSV with full VQA annotations to: ./mmstar_blind_224.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': 1500,\n",
       " 'question': 'How many chirality centers does the following molecule have?',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=340x266>,\n",
       " 'answer': 'A',\n",
       " 'category': 'science & technology',\n",
       " 'l2_category': 'biology & chemistry & physics',\n",
       " 'meta_info': {'source': 'MMMU',\n",
       "  'split': 'val',\n",
       "  'image_path': 'images/1499.jpg'},\n",
       " 'A': 'A',\n",
       " 'B': 'B',\n",
       " 'C': 'C',\n",
       " 'D': 'D',\n",
       " 'image_path': '/home/work/yuna/HPA/data/blind_224.png'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelscope_tsv(dataset, './mmstar_blind_224.tsv', image_folder='blind_224') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_question_and_options(question_text):\n",
    "    \"\"\"Extract clean question and individual options from question text\"\"\"\n",
    "    # Remove <image X> tokens\n",
    "    question_clean = re.sub(r'<image\\s+\\d+>', '', question_text)\n",
    "    \n",
    "    # Remove \\n characters\n",
    "    question_clean = question_clean.replace('\\n', ' ')\n",
    "    \n",
    "    # Split by \"Options:\" to separate question and options\n",
    "    parts = question_clean.split('Options:', 1)\n",
    "    question_only = parts[0].strip()\n",
    "    \n",
    "    # Initialize options\n",
    "    options = {'A': '', 'B': '', 'C': '', 'D': ''}\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        options_text = parts[1].strip()\n",
    "        # Extract options using regex (handles \"A:\", \"B:\", etc.)\n",
    "        option_pattern = r'([A-D]):\\s*([^,]+?)(?=,\\s*[A-D]:|$)'\n",
    "        matches = re.findall(option_pattern, options_text)\n",
    "        \n",
    "        for letter, value in matches:\n",
    "            # Clean up the value and skip 'nan'\n",
    "            value = value.strip()\n",
    "            if value.lower() != 'nan':\n",
    "                options[letter] = value\n",
    "    \n",
    "    return question_only, options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os \n",
    "import json \n",
    "\n",
    "def modelscope_tsv(items, output_path, image_folder='/home/work/yuna/VLMEval/data/val2014'):\n",
    "    \"\"\"\n",
    "    Write a list of items to a TSV file with columns:\n",
    "    index, answer, question, image_path\n",
    "\n",
    "    index\tcategory\tanswer\tquestion\tA\tB\tC\tD\timage_path\n",
    "    1\tAnimals\tA\tWhat animal is this?\tDog\tCat\tTiger\tElephant\t/root/LMUData/images/custom_mcq/dog.jpg\n",
    "    2\tBuildings\tD\tWhat building is this?\tSchool\tHospital\tPark\tMuseum\t/root/LMUData/images/custom_mcq/AMNH.jpg\n",
    "    3\tCities\tB\tWhich city's skyline is this?\tNew York\tTokyo\tShanghai\tParis\t/root/LMUData/images/custom_mcq/tokyo.jpg\n",
    "    4\tVehicles\tC\tWhat is the brand of this car?\tBMW\tAudi\tTesla\tMercedes\t/root/LMUData/images/custom_mcq/tesla.jpg\n",
    "    5\tActivities\tA\tWhat is the person in the picture doing?\tRunning\tSwimming\tReading\tSinging\t/root/LMUData/images/custom_mcq/running.jpg\n",
    "\n",
    "    Parameters:\n",
    "        data: list of dicts, each containing:\n",
    "              {\n",
    "                  \"answer\": str,\n",
    "                  \"question\": str,\n",
    "                  \"image_path\": str\n",
    "              }\n",
    "        output_path: str, path to write .tsv file\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        if items:  # Check if items list is not empty\n",
    "            # Get fieldnames from the first item after adding the new fields\n",
    "            first_item = items[0].copy()\n",
    "            first_item[\"index\"] = 1\n",
    "            if 'blind' in image_folder:\n",
    "                first_item[\"image_path\"] = f\"/home/work/yuna/HPA/data/{image_folder}.png\"\n",
    "            else:\n",
    "                first_item[\"image_path\"] = os.path.join(image_folder, item[\"image_id\"])\n",
    "            \n",
    "            choices = ['A', 'B', 'C', 'D'] \n",
    "            fieldnames = list(first_item.keys()) + choices \n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=\"\\t\")\n",
    "            writer.writeheader()\n",
    "\n",
    "            for idx, item in enumerate(items, start=1):\n",
    "                item['question'], options = extract_question_and_options(item['question'])\n",
    "                for option, choice in zip(options, choices): \n",
    "                    item[choice] = option \n",
    "                item[\"index\"] = idx\n",
    "                if 'blind' in image_folder:\n",
    "                    item[\"image_path\"] = f\"/home/work/yuna/HPA/data/{image_folder}.png\"\n",
    "                else:\n",
    "                    item[\"image_path\"] = os.path.join(image_folder, item[\"image_id\"])\n",
    "                writer.writerow(item)\n",
    "        \n",
    "    print(f\"Saved TSV with full VQA annotations to: {output_path}\")\n",
    "    return item "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacyovod_new_copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
