{"index": "0", "question": "Which option describe the object relationship in the image correctly?\nOptions: A: The suitcase is on the book., B: The suitcase is beneath the cat., C: The suitcase is beneath the bed., D: The suitcase is beneath the book.", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/0.jpg'}", "output": "d"}
{"index": "1", "question": "What is the main feature in the background of the image?\nOptions: A: A park bench near the water., B: A couple sitting on a bench., C: A body of water and the Golden Gate Bridge., D: A mountain in the distance.", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/1.jpg'}", "output": "c"}
{"index": "2", "question": "What seems to be the theme of the image?\nOptions: A: Hanging Posters, B: Music, C: Home decoration, D: Playing guitar", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/2.jpg'}", "output": "b"}
{"index": "3", "question": "What is the most prominent feature in the image?\nOptions: A: The skyline, B: The golf course, C: The trees, D: The person", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/3.jpg'}", "output": "b"}
{"index": "4", "question": "What is the center of focus in the image?\nOptions: A: A man writing in a book, B: A boy with his head in his hands surrounded by books, C: A cluttered desk with books and a pen, D: A stack of books covering a young man's face", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/4.jpg'}", "output": "b"}
{"index": "5", "question": "What is the main subject of the flyer seen in the image?\nOptions: A: Hair products, B: Makeup products, C: Spa services, D: Skin products", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/5.jpg'}", "output": "c"}
{"index": "6", "question": "What is the overall theme of the image?\nOptions: A: A woman in different polka dot dresses, B: A stone wall along a stone path, C: A woman in an outdoor setting, D: A house with a black roof", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/6.jpg'}", "output": "c"}
{"index": "7", "question": "What is the primary subject in this image?\nOptions: A: a guitar, B: a man on stage, C: a microphone, D: a concert", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/7.jpg'}", "output": "b"}
{"index": "8", "question": "What is the overall theme of the image?\nOptions: A: A businessman with a unique face, B: A man in a formal dress with a beard, C: An actor with a cut on his face, D: A man with a diagram of his face", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/8.jpg'}", "output": "d"}
{"index": "9", "question": "What feeling is represented in this image?\nOptions: A: engaged, B: distressed, C: angry, D: sad", "answer": "D", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/9.jpg'}", "output": "b"}
{"index": "10", "question": "What time of day is it in the image?\nOptions: A: Noon, B: Evening, C: Night, D: Dawn", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/10.jpg'}", "output": "b"}
{"index": "11", "question": "What is the dominant feature in the image?\nOptions: A: The bridge, B: The lake, C: The city skyline, D: The buildings", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/11.jpg'}", "output": "d"}
{"index": "12", "question": "What is the main focus of this image?\nOptions: A: The grassy hill, B: The sculpture, C: The fence, D: The building", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/12.jpg'}", "output": "c"}
{"index": "13", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Cozy, C: Happy, D: Angry", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/13.jpg'}", "output": "c"}
{"index": "14", "question": "What is the main subject of the image?\nOptions: A: Sky, B: Water, C: Sun, D: Clouds", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/14.jpg'}", "output": "c"}
{"index": "15", "question": "What object dominates the image in the foreground?\nOptions: A: A rocky and grassy field, B: Hills in the distance, C: A dirt road leading up a grassy hill to a rocky path, D: A large grey rock", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/15.jpg'}", "output": "c"}
{"index": "16", "question": "What feeling is shown in this image?\nOptions: A: engaged, B: distressed, C: angry, D: love", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/16.jpg'}", "output": "d"}
{"index": "17", "question": "What is the overall theme of the image?\nOptions: A: Beach vacation, B: Athletic lifestyle, C: Summer fashion, D: Urban street style", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/17.jpg'}", "output": "d"}
{"index": "18", "question": "What emotion is illustrated in this image?\nOptions: A: love, B: anger, C: happy, D: sad", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/18.jpg'}", "output": ""}
{"index": "19", "question": "<image 1> Which of the following is the most accurate statement of the cartoonist's meaning? \nOptions: A: World War I is the crime of the ages, B: ALl nations involved in World War I at its outbreak are to blame for it, C: Russia and Austria are blamed unfairly, D: The nations of Europe are blaming one another", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMMU', 'split': 'val', 'image_path': 'images/19.jpg'}", "output": "d"}
{"index": "20", "question": "What is the primary focus of the image?\nOptions: A: A woman riding a horse on a beach at sunset, B: A man riding a horse on a beach at sunset, C: A couple riding horses on a beach at sunset, D: A horse on a beach at sunset", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/20.jpg'}", "output": ""}
{"index": "21", "question": "What is the overall theme of this image?\nOptions: A: Sports, B: Sunset, C: Nature, D: Kids playing", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/21.jpg'}", "output": "d"}
{"index": "22", "question": "What is the main color scheme of the image?\nOptions: A: Shades of blue and green, B: Bright oranges and pinks, C: Various shades of gray and black, D: A rainbow of vivid colors", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/22.jpg'}", "output": ""}
{"index": "23", "question": "What is the overall mood of the image?\nOptions: A: Neutral, B: Somber, C: Tense, D: Joyful", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/23.jpg'}", "output": "d"}
{"index": "24", "question": "Which action is performed in this image?\nOptions: A: marching, B: playing cymbals, C: long jump, D: cheerleading", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/24.jpg'}", "output": "c"}
{"index": "25", "question": "What is the weather like in the image?\nOptions: A: Snowy, B: Cloudy, C: Rainy, D: Sunny", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/25.jpg'}", "output": "d"}
{"index": "26", "question": "What is the main focus of the image?\nOptions: A: the woman's phone, B: the woman's hairstyle, C: the woman's sweater, D: the background scenery", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/26.jpg'}", "output": ""}
{"index": "27", "question": "What is happening in the image?\nOptions: A: A group of men are playing soccer on a field., B: A football player is kicking a ball., C: A soccer player is practicing ball control at the stadium., D: An Olympic athlete is competing in a soccer match.", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/27.jpg'}", "output": "b"}
{"index": "28", "question": "What is the primary activity depicted in the image?\nOptions: A: A crane lifting a box from a building, B: A man on a truck operating a crane to lift a box from a building, C: A crane lifting a house at sunset, D: A man on a balcony of a house under construction in the city at sunset", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/28.jpg'}", "output": ""}
{"index": "29", "question": "What is the main purpose of this garden?\nOptions: A: A venue for outdoor events, B: A place for sports activities, C: A spot for quiet and peaceful recreation, D: A location for plant research and study", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/29.jpg'}", "output": "c"}
{"index": "30", "question": "What is the prominent feature of the setting in the image?\nOptions: A: A bright ceiling light, B: A glass window, C: A large white curtain, D: A black speaker", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/30.jpg'}", "output": "c"}
{"index": "31", "question": "Which of the following words best describes the theme of this image?\nOptions: A: Salad, B: Fruit, C: Healthy, D: Chicken", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/31.jpg'}", "output": "c"}
{"index": "32", "question": "What is the predominant feature in the image?\nOptions: A: Cars, B: People, C: Buildings, D: Skyscrapers", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/32.jpg'}", "output": "c"}
{"index": "33", "question": "What kind of environment is depicted in the image?\nOptions: A: A historical site, B: A religious location, C: A residential area, D: A garden", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/33.jpg'}", "output": "d"}
{"index": "34", "question": "What is the dominant color in the image?\nOptions: A: Red, B: Blue, C: Green, D: Black", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/34.jpg'}", "output": "d"}
{"index": "35", "question": "What is the dominant color in the image?\nOptions: A: White, B: Blue, C: Brown, D: Black", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/35.jpg'}", "output": "b"}
{"index": "36", "question": "What is the overall scene depicted in this image?\nOptions: A: A group of people ice skating on a frozen lake surrounded by snowy trees and hills, B: A small town during a winter snowfall with snow-covered buildings and trees, C: A busy city street during Christmas time with decorated buildings and light displays, D: A ski resort town with snowy mountains and ski lifts visible in the background", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/36.jpg'}", "output": "b"}
{"index": "37", "question": "What is the predominant color in this image?\nOptions: A: Blue, B: White, C: Brown, D: Green", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/37.jpg'}", "output": "c"}
{"index": "38", "question": "Which of the following best describes the view from the deck?\nOptions: A: A view of a forest, B: A view of the cityscape, C: A view of the mountains, D: A view of a body of water", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/38.jpg'}", "output": "c"}
{"index": "39", "question": "What is the subject of the image?\nOptions: A: A person on stage playing guitar, B: A group of people playing music on stage, C: A group of people on stage playing guitars and singing, D: A group of people on stage playing music at night", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/39.jpg'}", "output": "b"}
{"index": "40", "question": "What type of scene is depicted in this image?\nOptions: A: A workspace, B: A beach vacation, C: A tropical restaurant, D: A desert oasis", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/40.jpg'}", "output": "b"}
{"index": "41", "question": "What is the general theme of this image?\nOptions: A: Candy buffet, B: Ice cream sundae, C: Yogurt containers, D: Fruit desserts", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/41.jpg'}", "output": "b"}
{"index": "42", "question": "Which image shows the highest contrast?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/42.jpg'}", "output": ""}
{"index": "43", "question": "Based on the image, which statement best describes the likely purpose of the setting?\nOptions: A: A photoshoot for greeting cards, B: A scientific research site, C: A festive celebration, D: A picnic spot for a group of people", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/43.jpg'}", "output": ""}
{"index": "44", "question": "What kind of sport is being played in this image?\nOptions: A: Soccer, B: Basketball, C: Football (American), D: Rugby", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/44.jpg'}", "output": "d"}
{"index": "45", "question": "What is the main focus of the image?\nOptions: A: A person playing a guitar in front of a choir, B: A choir singing on a stage, C: A man sitting in a chair playing a guitar, D: A man playing a guitar in front of a group of people", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/45.jpg'}", "output": "c"}
{"index": "46", "question": "Which image shows the highest sharpness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/46.jpg'}", "output": ""}
{"index": "47", "question": "What kind of weather is depicted in the picture?\nOptions: A: sunny, B: rainy, C: windy, D: snowy", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/47.jpg'}", "output": "c"}
{"index": "48", "question": "What is the main focus of the image?\nOptions: A: The sidewalk, B: The shoes, C: The ballerina, D: The studio", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/48.jpg'}", "output": "c"}
{"index": "49", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/49.jpg'}", "output": ""}
{"index": "50", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/50.jpg'}", "output": "b"}
{"index": "51", "question": "what style is depicted in this image?\nOptions: A: impressionism, B: post-Impressionism, C: modernism, D: dadaism", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/51.jpg'}", "output": "d"}
{"index": "52", "question": "What is the main focus of the image?\nOptions: A: A fountain with a horse and dragon statue, B: A building with sculptures in the front yard, C: A park with a statue of a man on a horse, D: A statue of a lion and a dragon in front of a building", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/52.jpg'}", "output": "c"}
{"index": "53", "question": "Which emotion is being depicted in this image?\nOptions: A: happiness, B: sadness, C: anger, D: loneliness", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/53.jpg'}", "output": "b"}
{"index": "54", "question": "Which term matches the picture?\nOptions: A: basket star, B: brittle star, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/54.jpg'}", "output": ""}
{"index": "55", "question": "What is the main focus of the image?\nOptions: A: A woman and two girls standing on a red carpet with an umbrella., B: A person and their family at a premiere standing under an umbrella in the rain., C: Two girls holding an umbrella on a red carpet., D: People standing on a red carpet and an actress holding her daughter under an umbrella.", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/55.jpg'}", "output": ""}
{"index": "56", "question": "What color dominates the logo seen on the wall?\nOptions: A: Red, B: Black and White, C: Yellow, D: Blue", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/56.jpg'}", "output": "b"}
{"index": "57", "question": "What is the main theme of the image?\nOptions: A: Nature, B: Transportation, C: Travel, D: Outdoor recreation", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/57.jpg'}", "output": "c"}
{"index": "58", "question": "Based on the image, what could be an appropriate title for this scene?\nOptions: A: \"A Peaceful Countryside Gathering\", B: \"A Bustling City Square\", C: \"A Day at the Beach\", D: \"A Mountain Adventure\"", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/58.jpg'}", "output": "b"}
{"index": "59", "question": "What is the main color of the image?\nOptions: A: Red, B: Green, C: Black, D: White", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/59.jpg'}", "output": "d"}
{"index": "60", "question": "What is the main color scheme of this image?\nOptions: A: Black and silver, B: White and blue, C: Brown and red, D: Red and black", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/60.jpg'}", "output": "b"}
{"index": "61", "question": "What is the overall mood of this image?\nOptions: A: Excitement and liveliness, B: Fear and anxiety, C: Calmness and relaxation, D: Confusion and chaos", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/61.jpg'}", "output": "c"}
{"index": "62", "question": "What is the main feature of this image?\nOptions: A: A sailboat on its side in the ocean at sunset, B: A sailboat in the water with the sun over the ocean and rocks, C: A broken sailboat resting on a rocky shore during sunset, D: A rocky shore with a sailboat in the distance at sunrise", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/62.jpg'}", "output": "c"}
{"index": "63", "question": "What is the prominent color in the image?\nOptions: A: Blue, B: Green, C: Grey, D: White", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/63.jpg'}", "output": "b"}
{"index": "64", "question": "What is the main feature of the image?\nOptions: A: People on the rooftop of a building, B: A large area of ferns, C: A garden with trees, plants and a building, D: A blue cloudy sky", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/64.jpg'}", "output": "b"}
{"index": "65", "question": "What is the dominant color in the image?\nOptions: A: Yellow, B: Black, C: Red, D: Green", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/65.jpg'}", "output": "d"}
{"index": "66", "question": "What is the main feature of the pictured place?\nOptions: A: Large windows with views of the ocean, B: A swimming pool in the living room, C: A decorative green bench, D: A mountain in the distance", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/66.jpg'}", "output": "b"}
{"index": "67", "question": "What is the dominant color scheme in this image?\nOptions: A: White and blue, B: Blue and green, C: Brown and yellow, D: Pink and orange", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/67.jpg'}", "output": "b"}
{"index": "68", "question": "What is the overall mood or atmosphere of the image?\nOptions: A: Joyful and energetic, B: Intimate and cozy, C: Dark and gloomy, D: Serene and peaceful", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/68.jpg'}", "output": ""}
{"index": "69", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "D", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/69.jpg'}", "output": "d"}
{"index": "70", "question": "What is the main object present in the foreground of the image?\nOptions: A: A surgical mask, B: A nurse, C: A surgical light, D: A surgical gown", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/70.jpg'}", "output": "b"}
{"index": "71", "question": "What is the primary scene depicted in the image?\nOptions: A: A group of people taking pictures of a statue in a plaza, B: A large crowd taking pictures of each other in an outdoor event, C: A group of people taking pictures of a Christmas tree in a plaza, D: A group of people taking pictures of a famous building in New York City", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/71.jpg'}", "output": "c"}
{"index": "72", "question": "What kind of room is depicted in the image?\nOptions: A: A living room, B: A kitchen, C: A bedroom, D: A bathroom", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/72.jpg'}", "output": ""}
{"index": "73", "question": "Which image shows the highest colorfulness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/73.jpg'}", "output": "d"}
{"index": "74", "question": "What is the predominant sport being played in the image?\nOptions: A: football, B: basketball, C: soccer, D: baseball", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/74.jpg'}", "output": "c"}
{"index": "75", "question": "What is the predominant feature of the forest in the image?\nOptions: A: The ground is dry and covered in brown dirt, B: The trees are all tall and bare with no leaves, C: The trees are covered in green ivy and climbing vines, D: The bark on the trees is white and peeling", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/75.jpg'}", "output": "b"}
{"index": "76", "question": "What is the main theme of the image?\nOptions: A: Bearded men, B: Bathroom hygiene, C: Royal monarchy, D: Home decor", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/76.jpg'}", "output": ""}
{"index": "77", "question": "What is happening in the image?\nOptions: A: A football player is tackling a soccer player, B: A soccer player is kicking a football, C: A group of men are playing soccer, D: A football player is dribbling the ball towards the goal", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/77.jpg'}", "output": ""}
{"index": "78", "question": "What is the color theme of the bathroom?\nOptions: A: Gray and white, B: Beige and white, C: Black and white, D: Brown and white", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/78.jpg'}", "output": ""}
{"index": "79", "question": "Where is the largest body of water in the image located?\nOptions: A: At the center of the image, B: On the bottom of the image, C: On the left side of the image, D: On the right side of the image", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/79.jpg'}", "output": "d"}
{"index": "80", "question": "What is the dominant color of the clothing worn by people in the image?\nOptions: A: White, B: Green, C: Red, D: Blue", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/80.jpg'}", "output": ""}
{"index": "81", "question": "What is the most frequent color shown in the image?\nOptions: A: White, B: Brown, C: Blue, D: Green", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/81.jpg'}", "output": "d"}
{"index": "82", "question": "What is the overall theme of the image?\nOptions: A: Fashion, B: Wildlife, C: Nature, D: Photography", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/82.jpg'}", "output": "d"}
{"index": "83", "question": "What is the main object in the image?\nOptions: A: The guitar, B: The man, C: The woman, D: The person on stage", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/83.jpg'}", "output": ""}
{"index": "84", "question": "What is the primary focus of the people in the image?\nOptions: A: Watching a performance, B: Standing in lines for a concert, C: Participating in a protest, D: Walking down the street", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/84.jpg'}", "output": "d"}
{"index": "85", "question": "What is the most prominent feature in the image?\nOptions: A: Sailboats, B: City skyline, C: Sky, D: Sun", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/85.jpg'}", "output": "d"}
{"index": "86", "question": "What is the most prominent feature in the image?\nOptions: A: a blue fence, B: a gate, C: a palm tree, D: a green wall", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/86.jpg'}", "output": ""}
{"index": "87", "question": "What is the dominant color of the image?\nOptions: A: White, B: Green, C: Gray, D: Blue", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/87.jpg'}", "output": "d"}
{"index": "88", "question": "What is the main focus of the image?\nOptions: A: A woman standing in front of a brick wall, B: A woman standing in front of a green hedge, C: A garden with a lot of flowers and bushes, D: A bench surrounded by a lot of plants", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/88.jpg'}", "output": "b"}
{"index": "89", "question": "Which object is the most central in the image?\nOptions: A: Woman in white dress, B: Actor in a white dress on the red carpet at a festival, C: Refrigerator, D: Red Carpet", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/89.jpg'}", "output": "d"}
{"index": "90", "question": "What is the dominant color in the picture?\nOptions: A: Pink, B: Gray, C: Blue, D: Green", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/90.jpg'}", "output": "d"}
{"index": "91", "question": "What is the main subject of the image?\nOptions: A: A woman, B: A bed, C: A man and a woman, D: A girl", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/91.jpg'}", "output": ""}
{"index": "92", "question": "Which category does this image belong to?\nOptions: A: medical CT image, B: 8-bit, C: digital art, D: painting", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/92.jpg'}", "output": "b"}
{"index": "93", "question": "What is the primary subject of the image?\nOptions: A: The sunrise, B: The grassy field, C: The trees, D: The fog", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/93.jpg'}", "output": "b"}
{"index": "94", "question": "What is the color of the majority of objects in the image?\nOptions: A: White, B: Gray, C: Blue, D: Green", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/94.jpg'}", "output": "b"}
{"index": "95", "question": "what style is depicted in this image?\nOptions: A: impressionism, B: post-Impressionism, C: modernism, D: dadaism", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/95.jpg'}", "output": "b"}
{"index": "96", "question": "What is the focus of the image?\nOptions: A: A traffic sign, B: A no entry sign, C: A group of trees, D: A lake", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/96.jpg'}", "output": "b"}
{"index": "97", "question": "Which mood does this image convey?\nOptions: A: Cozy, B: Anxious, C: Happy, D: Angry", "answer": "C", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/97.jpg'}", "output": "c"}
{"index": "98", "question": "How many times is the object 'tree' detected in the attribute detections?\nOptions: A: Two, B: Three, C: Four, D: Five", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/98.jpg'}", "output": "b"}
{"index": "99", "question": "What is the main color theme of the scene?\nOptions: A: Red, B: Green, C: Blue, D: Yellow", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/99.jpg'}", "output": "d"}
{"index": "100", "question": "Which is the main topic of the image\nOptions: A: A woman surfing, B: A man skiting, C: A man surfing, D: A woman skiting", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/100.jpg'}", "output": "c"}
{"index": "101", "question": "What is the dominant color of the room?\nOptions: A: White, B: Green, C: Blue, D: Red", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/101.jpg'}", "output": "c"}
{"index": "102", "question": "Does the image depict a balcony?\nOptions: A: Yes, B: No, C: Cannot be determined, D: Not enough information", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/102.jpg'}", "output": ""}
{"index": "103", "question": "What is the main feature in the given image?\nOptions: A: Green plants, B: Fog, C: Bushes, D: Trees", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/103.jpg'}", "output": ""}
{"index": "104", "question": "What is the dominant color in the image?\nOptions: A: Blue, B: Yellow, C: Red, D: Green", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/104.jpg'}", "output": "b"}
{"index": "105", "question": "Which object can be found in the center of the image?\nOptions: A: Goalpost, B: Soccer ball, C: Referee, D: Crowd of people", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/105.jpg'}", "output": "c"}
{"index": "106", "question": "Which corner doesn't have any plates?\nOptions: A: top-right, B: top-left, C: bottom-left, D: bottom-right", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/106.jpg'}", "output": "d"}
{"index": "107", "question": "Which corner doesn't have any plates?\nOptions: A: top-right, B: top-left, C: bottom-left, D: bottom-right", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/107.jpg'}", "output": "c"}
{"index": "108", "question": "What is the relative position of the man and the child in this image?\nOptions: A: The man and the child are out of frame, B: The man is standing next to the child, C: The man is holding the child, D: The man is standing apart from the child", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/108.jpg'}", "output": "b"}
{"index": "109", "question": "How many different \"pointed\" kinds are there?\nOptions: A: 2, B: 4, C: 1, D: 3", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/109.jpg'}", "output": "d"}
{"index": "110", "question": "Which of the following objects is the closest to the apple in the image?\nOptions: A: Grilled caramel apple crunch on tin foil, B: Small jar in the middle, C: White and black wax paper, D: None of the above", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/110.jpg'}", "output": ""}
{"index": "111", "question": "What is the predominant color of the image?\nOptions: A: Yellow, B: Black, C: White, D: Blue", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/111.jpg'}", "output": "d"}
{"index": "112", "question": "What is the primary focus of the image?\nOptions: A: The boats in the harbor, B: The castle on the island, C: The cloudy sky, D: The city on the coast", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/112.jpg'}", "output": "b"}
{"index": "113", "question": "Which country is highlighted?\nOptions: A: Saint Vincent and the Grenadines, B: Grenada, C: the Dominican Republic, D: Trinidad and Tobago", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/113.jpg'}", "output": "b"}
{"index": "114", "question": "What represents hazelnuts in the figure?\nOptions: A: D, B: G, C: C, D: A", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/114.jpg'}", "output": "c"}
{"index": "115", "question": "What is the main feature of the building in the image?\nOptions: A: The rooftop, B: The stone wall, C: The clock tower, D: The arched windows", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/115.jpg'}", "output": "c"}
{"index": "116", "question": "What type of event is depicted in the image?\nOptions: A: A political rally, B: A sports game, C: A protest march, D: A music festival", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/116.jpg'}", "output": "b"}
{"index": "117", "question": "What is the main focus of the image?\nOptions: A: The wooden frame with a variety of plants, B: The mirrors and frames on the wall, C: The planter hanging on the wooden wall, D: The color scheme of the image", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/117.jpg'}", "output": "c"}
{"index": "118", "question": "What is the occasion depicted in the image?\nOptions: A: New Year's Eve celebration, B: Christmas Eve party, C: Birthday celebration, D: Wedding reception", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/118.jpg'}", "output": "b"}
{"index": "119", "question": "What is the weather like in this image?\nOptions: A: Cloudy, B: Rainy, C: Clear, D: Windy", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/119.jpg'}", "output": "d"}
{"index": "120", "question": "How many police officers are in the image?\nOptions: A: 1, B: 0, C: 2, D: 3", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/120.jpg'}", "output": ""}
{"index": "121", "question": "What is the main activity happening in this image?\nOptions: A: A couple of men are playing soccer, B: A group of people are having a picnic in the park, C: Two men are playing cricket, D: A group of people are playing volleyball", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/121.jpg'}", "output": ""}
{"index": "122", "question": "What is the predominant feature in the image?\nOptions: A: Houses, B: The Sky, C: Mountains, D: Trees", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/122.jpg'}", "output": ""}
{"index": "123", "question": "Where is the exit in the image?\nOptions: A: To the left of the green exit sign, B: To the right of the green exit sign, C: Behind the green exit sign with an arrow pointing to the left, D: Not visible in the image", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/123.jpg'}", "output": ""}
{"index": "124", "question": "Which is the main topic of the image\nOptions: A: Driving cars, B: Driving buses, C: A driving bus, D: A driving car", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/124.jpg'}", "output": "b"}
{"index": "125", "question": "Which is the main topic of the image\nOptions: A: A toy bear and a toy dog, B: A toy bear and a toy chicken, C: A toy bear and a toy cat, D: A toy bear and a toy rabbit", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/125.jpg'}", "output": "b"}
{"index": "126", "question": "What is the general theme of the image?\nOptions: A: Beach party, B: Fair or festival, C: Wedding celebration, D: Night market", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/126.jpg'}", "output": "d"}
{"index": "127", "question": "What is the predominant feature of the image?\nOptions: A: Stage, B: People, C: Guitars, D: Dark Room", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/127.jpg'}", "output": "c"}
{"index": "128", "question": "What is the main event in the image?\nOptions: A: A theater play, B: A sporting event, C: A concert, D: A dance competition", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/128.jpg'}", "output": "d"}
{"index": "129", "question": "What is the dominant color of the image?\nOptions: A: White, B: Brown, C: Gray, D: Green", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/129.jpg'}", "output": "d"}
{"index": "130", "question": "What is the background of the image?\nOptions: A: A beach, B: A city street, C: A forest, D: A meadow", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/130.jpg'}", "output": "c"}
{"index": "131", "question": "What is the dominant color palette of the image?\nOptions: A: Red and black, B: Green and brown, C: White and pink, D: Blue and gray", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/131.jpg'}", "output": ""}
{"index": "132", "question": "What is the predominant color in the image?\nOptions: A: White, B: Red, C: Blue, D: Silver", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/132.jpg'}", "output": "c"}
{"index": "133", "question": "What is the intended mood of this living room?\nOptions: A: Cozy and comfy, B: Bright and sunny, C: Dark and dramatic, D: Sleek and modern", "answer": "D", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/133.jpg'}", "output": ""}
{"index": "134", "question": "What is the weather like in the image?\nOptions: A: Cloudy and cool, B: Sunny and hot, C: Windy and stormy, D: Hazy and humid", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/134.jpg'}", "output": ""}
{"index": "135", "question": "What is the primary focal point of this image?\nOptions: A: A dirt road lined with trees, B: A pathway through a park, C: A tree-lined pathway leading to a bench, D: A fence and gate leading to a driveway", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/135.jpg'}", "output": "c"}
{"index": "136", "question": "What is the sport being played in the image?\nOptions: A: Tennis, B: Soccer, C: Volleyball, D: Basketball", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/136.jpg'}", "output": "b"}
{"index": "137", "question": "What is the most dominant color in the image?\nOptions: A: Black, B: Blue, C: Red, D: White", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/137.jpg'}", "output": "b"}
{"index": "138", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "D", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/138.jpg'}", "output": "d"}
{"index": "139", "question": "What emotion is portrayed in this image?\nOptions: A: happiness, B: sadness, C: anger, D: loneliness", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/139.jpg'}", "output": "d"}
{"index": "140", "question": "Which of the following emotions is shown in this image?\nOptions: A: weavy, B: lonely, C: happy, D: supportive", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/140.jpg'}", "output": "b"}
{"index": "141", "question": "Which image shows the highest contrast?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/141.jpg'}", "output": "d"}
{"index": "142", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/142.jpg'}", "output": ""}
{"index": "143", "question": "Which image shows the highest sharpness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/143.jpg'}", "output": "b"}
{"index": "144", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/144.jpg'}", "output": ""}
{"index": "145", "question": "Which image shows the highest sharpness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/145.jpg'}", "output": ""}
{"index": "146", "question": "Which image shows the highest contrast?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/146.jpg'}", "output": "b"}
{"index": "147", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/147.jpg'}", "output": "d"}
{"index": "148", "question": "Which image shows the highest sharpness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/148.jpg'}", "output": ""}
{"index": "149", "question": "Which image shows the highest contrast?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/149.jpg'}", "output": "c"}
{"index": "150", "question": "Which image shows the highest colorfulness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/150.jpg'}", "output": "c"}
{"index": "151", "question": "Does the picture show a mountainous landscape or a coastal landscape?\nOptions: A: Mountainous, B: Coastal, C: plain, D: basin", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/151.jpg'}", "output": ""}
{"index": "152", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/152.jpg'}", "output": "c"}
{"index": "153", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/153.jpg'}", "output": "b"}
{"index": "154", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/154.jpg'}", "output": "b"}
{"index": "155", "question": "Which category does this image belong to?\nOptions: A: oil painting, B: sketch, C: digital art, D: photo", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/155.jpg'}", "output": "b"}
{"index": "156", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/156.jpg'}", "output": ""}
{"index": "157", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/157.jpg'}", "output": "b"}
{"index": "158", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/158.jpg'}", "output": ""}
{"index": "159", "question": "what style is depicted in this image?\nOptions: A: impressionism, B: post-Impressionism, C: modernism, D: dadaism", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/159.jpg'}", "output": "b"}
{"index": "160", "question": "Which category does this image belong to?\nOptions: A: medical CT image, B: 8-bit, C: digital art, D: painting", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/160.jpg'}", "output": "b"}
{"index": "161", "question": "Identify the art style of this image.\nOptions: A: oil paint, B: vector art, C: Baroque, D: watercolor", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/161.jpg'}", "output": "d"}
{"index": "162", "question": "what style is depicted in this image?\nOptions: A: impressionism, B: post-Impressionism, C: modernism, D: dadaism", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/162.jpg'}", "output": ""}
{"index": "163", "question": "Which category does this image belong to?\nOptions: A: oil painting, B: sketch, C: digital art, D: photo", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/163.jpg'}", "output": "b"}
{"index": "164", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/164.jpg'}", "output": ""}
{"index": "165", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/165.jpg'}", "output": ""}
{"index": "166", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/166.jpg'}", "output": ""}
{"index": "167", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/167.jpg'}", "output": ""}
{"index": "168", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/168.jpg'}", "output": ""}
{"index": "169", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/169.jpg'}", "output": "b"}
{"index": "170", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/170.jpg'}", "output": "b"}
{"index": "171", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/171.jpg'}", "output": ""}
{"index": "172", "question": "is this place crowded?\nOptions: A: yes, B: no, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/172.jpg'}", "output": "b"}
{"index": "173", "question": "The image displays which art style?\nOptions: A: watercolor, B: early renaissance, C: art nouveau, D: vector art", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/173.jpg'}", "output": ""}
{"index": "174", "question": "What feeling is represented in this image?\nOptions: A: engaged, B: disordered, C: angry, D: supportive", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/174.jpg'}", "output": "b"}
{"index": "175", "question": "Can you identify the season in which the picture was taken?\nOptions: A: spring, B: summer, C: fall, D: winter", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/175.jpg'}", "output": "b"}
{"index": "176", "question": "Which category does this image belong to?\nOptions: A: oil painting, B: sketch, C: digital art, D: photo", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/176.jpg'}", "output": "b"}
{"index": "177", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/177.jpg'}", "output": ""}
{"index": "178", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/178.jpg'}", "output": ""}
{"index": "179", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/179.jpg'}", "output": ""}
{"index": "180", "question": "Which is the main topic of the image\nOptions: A: A boy skiting, B: A girl skiting, C: A man skiting, D: A woman skiting", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/180.jpg'}", "output": "c"}
{"index": "181", "question": "Which emotion is being depicted in this image?\nOptions: A: happiness, B: sadness, C: anger, D: loneliness", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/181.jpg'}", "output": "b"}
{"index": "182", "question": "Which emotion is being depicted in this image?\nOptions: A: happiness, B: sadness, C: anger, D: love", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/182.jpg'}", "output": "b"}
{"index": "183", "question": "Which category does this image belong to?\nOptions: A: oil painting, B: sketch, C: digital art, D: photo", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/183.jpg'}", "output": "d"}
{"index": "184", "question": "Which style is represented in this image?\nOptions: A: photography, B: HDR, C: comic, D: pencil", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/184.jpg'}", "output": ""}
{"index": "185", "question": "what style is this painting?\nOptions: A: ink wash painting, B: watercolor painting, C: gouache painting, D: pen and ink", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/185.jpg'}", "output": "b"}
{"index": "186", "question": "what style is depicted in this image?\nOptions: A: impressionism, B: post-Impressionism, C: modernism, D: dadaism", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/186.jpg'}", "output": "b"}
{"index": "187", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/187.jpg'}", "output": ""}
{"index": "188", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/188.jpg'}", "output": ""}
{"index": "189", "question": "What feeling is shown in this image?\nOptions: A: engaged, B: lonely, C: angry, D: supportive", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/189.jpg'}", "output": "b"}
{"index": "190", "question": "This image is an example of which style?\nOptions: A: HDR, B: Baroque, C: oil paint, D: comic", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/190.jpg'}", "output": "c"}
{"index": "191", "question": "Which art style is showcased in this image?\nOptions: A: depth of field, B: pencil, C: vector art, D: Baroque", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/191.jpg'}", "output": "b"}
{"index": "192", "question": "This image is an example of which style?\nOptions: A: vector art, B: comic, C: oil paint, D: Baroque", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/192.jpg'}", "output": ""}
{"index": "193", "question": "What art style is evident in this image?\nOptions: A: watercolor, B: photography, C: vector art, D: pencil", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/193.jpg'}", "output": "c"}
{"index": "194", "question": "Which one is the correct caption of this image?\nOptions: A: a brown and black ox and a white and black one and grass, B: A beautiful woman holding up an umbrella next to a forest., C: A cutting board and a metal pan topped with pizza., D: A huge heard of sheep are all scattered together.", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/194.jpg'}", "output": "d"}
{"index": "195", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/195.jpg'}", "output": "b"}
{"index": "196", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/196.jpg'}", "output": ""}
{"index": "197", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/197.jpg'}", "output": "b"}
{"index": "198", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/198.jpg'}", "output": "b"}
{"index": "199", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/199.jpg'}", "output": "b"}
{"index": "200", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/200.jpg'}", "output": ""}
{"index": "201", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/201.jpg'}", "output": ""}
{"index": "202", "question": "what emotion does this emoji express?\nOptions: A: happy, B: sad, C: excited, D: angry", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/202.jpg'}", "output": "b"}
{"index": "203", "question": "what style is this painting?\nOptions: A: ink wash painting, B: watercolor painting, C: gouache painting, D: pen and ink", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/203.jpg'}", "output": "d"}
{"index": "204", "question": "Which is the main topic of the image\nOptions: A: A woman is playing tennis, B: A man is playing tennis, C: A boy is playing soccer, D: A girl is playing volleyball", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/204.jpg'}", "output": "b"}
{"index": "205", "question": "Which scene category matches this image the best?\nOptions: A: manufactured_home, B: campus, C: badlands, D: field/cultivated", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/205.jpg'}", "output": "b"}
{"index": "206", "question": "What motion this image want to convey?\nOptions: A: happy, B: angry, C: sad, D: terrified", "answer": "C", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/206.jpg'}", "output": "c"}
{"index": "207", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/207.jpg'}", "output": ""}
{"index": "208", "question": "What type of environment is depicted in the picture?\nOptions: A: home, B: shopping mall, C: street, D: forest", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/208.jpg'}", "output": "c"}
{"index": "209", "question": "Can you identify the season in which the picture was taken?\nOptions: A: spring, B: summer, C: fall, D: winter", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/209.jpg'}", "output": ""}
{"index": "210", "question": "Can you identify the season in which the picture was taken?\nOptions: A: spring, B: summer, C: fall, D: winter", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/210.jpg'}", "output": "b"}
{"index": "211", "question": "Can you identify the season in which the picture was taken?\nOptions: A: spring, B: summer, C: fall, D: winter", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/211.jpg'}", "output": ""}
{"index": "212", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/212.jpg'}", "output": ""}
{"index": "213", "question": "What kind of weather is depicted in the picture?\nOptions: A: sunny, B: rainy, C: windy, D: snowy", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/213.jpg'}", "output": ""}
{"index": "214", "question": "Can you identify the season in which the picture was taken?\nOptions: A: spring, B: summer, C: fall, D: winter", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/214.jpg'}", "output": "d"}
{"index": "215", "question": "what style is depicted in this image?\nOptions: A: impressionism, B: post-Impressionism, C: modernism, D: dadaism", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/215.jpg'}", "output": "d"}
{"index": "216", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/216.jpg'}", "output": ""}
{"index": "217", "question": "Which one is the correct caption of this image?\nOptions: A: a messy bed room a bed a chair and boxes, B: A woman laying in bed next to a large stuffed animal., C: A tennis player resting on the floor under a hat., D: Odd plant and flower arrangement in a vase.", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/217.jpg'}", "output": ""}
{"index": "218", "question": "Which category does this image belong to?\nOptions: A: oil painting, B: sketch, C: digital art, D: photo", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/218.jpg'}", "output": ""}
{"index": "219", "question": "Which category does this image belong to?\nOptions: A: oil painting, B: sketch, C: digital art, D: photo", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/219.jpg'}", "output": "d"}
{"index": "220", "question": "Which mood does this image convey?\nOptions: A: Cozy, B: Anxious, C: Happy, D: Angry", "answer": "C", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/220.jpg'}", "output": "c"}
{"index": "221", "question": "What emotion is portrayed in this image?\nOptions: A: happiness, B: sadness, C: anger, D: love", "answer": "C", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/221.jpg'}", "output": "c"}
{"index": "222", "question": "What style does this image represent?\nOptions: A: HDR, B: watercolor, C: comic, D: photograph", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/222.jpg'}", "output": "b"}
{"index": "223", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "D", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/223.jpg'}", "output": "d"}
{"index": "224", "question": "Which one is the correct caption of this image?\nOptions: A: A bunch of cars sitting still in the middle of a street, B: Two giraffes near a tree in the wild., C: Small personal bathroom with a tiny entrance door., D: An elephant drinking water while the rest of the herd is walking in dry grass.", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/224.jpg'}", "output": "d"}
{"index": "225", "question": "Which scene category matches this image the best?\nOptions: A: excavation, B: forest/broadleaf, C: botanical_garden, D: jewelry_shop", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/225.jpg'}", "output": "b"}
{"index": "226", "question": "Which mood does this image convey?\nOptions: A: Cozy, B: Anxious, C: Happy, D: Angry", "answer": "C", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/226.jpg'}", "output": "c"}
{"index": "227", "question": "Which of the following captions best describes this image?\nOptions: A: A person playing a guitar on a stage, B: A group of people dancing at a party, C: A singer performing on a microphone, D: A person playing a piano in a studio", "answer": "B", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/227.jpg'}", "output": "b"}
{"index": "228", "question": "Which category does this image belong to?\nOptions: A: MRI image, B: icon, C: microscopic image, D: abstract painting", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/228.jpg'}", "output": "d"}
{"index": "229", "question": "What kind of human behavior does this picture describe?\nOptions: A: A doctor is performing surgery in a hospital, using advanced techniques and technology to save lives and restore health to patients in need., B: A couple is horseback riding on a trail, cantering and galloping through scenic terrain while bonding with each other and their equine companions., C: A man in a cowboy hat sits sleeping in the sun with a fallen apple at his feet., D: A group of friends are playing poker in a basement, betting chips and bluffing while trying to outsmart each other.", "answer": "C", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/229.jpg'}", "output": "c"}
{"index": "230", "question": "Which category does this image belong to?\nOptions: A: MRI image, B: icon, C: microscopic image, D: abstract painting", "answer": "B", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/230.jpg'}", "output": "b"}
{"index": "231", "question": "Which category does this image belong to?\nOptions: A: MRI image, B: icon, C: microscopic image, D: abstract painting", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/231.jpg'}", "output": "c"}
{"index": "232", "question": "Which category does this image belong to?\nOptions: A: MRI image, B: icon, C: microscopic image, D: abstract painting", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/232.jpg'}", "output": "d"}
{"index": "233", "question": "what style is this painting?\nOptions: A: ink wash painting, B: watercolor painting, C: gouache painting, D: pen and ink", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/233.jpg'}", "output": "d"}
{"index": "234", "question": "Which of the following captions best describes this image?\nOptions: A: A person taking a photo with a camera, B: A group of people watching a movie in a theater, C: A person reading a book in a library, D: A woman applying makeup in front of a mirror", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/234.jpg'}", "output": "d"}
{"index": "235", "question": "what style is this painting?\nOptions: A: ink wash painting, B: watercolor painting, C: gouache painting, D: pen and ink", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/235.jpg'}", "output": "d"}
{"index": "236", "question": "what style is this painting?\nOptions: A: ink wash painting, B: watercolor painting, C: gouache painting, D: pen and ink", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/236.jpg'}", "output": "c"}
{"index": "237", "question": "what style is this painting?\nOptions: A: ink wash painting, B: watercolor painting, C: gouache painting, D: pen and ink", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/237.jpg'}", "output": "c"}
{"index": "238", "question": "Which category does this image belong to?\nOptions: A: remote sense image, B: photo, C: painting, D: map", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/238.jpg'}", "output": "d"}
{"index": "239", "question": "Which category does this image belong to?\nOptions: A: remote sense image, B: photo, C: painting, D: map", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/239.jpg'}", "output": "d"}
{"index": "240", "question": "Which category does this image belong to?\nOptions: A: remote sense image, B: photo, C: painting, D: map", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/240.jpg'}", "output": "d"}
{"index": "241", "question": "Which of the following captions best describes this image?\nOptions: A: A person holding a bouquet of flowers, B: A group of people eating at a restaurant, C: A person playing with a pet dog, D: A woman getting a pedicure at a salon", "answer": "D", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/241.jpg'}", "output": "d"}
{"index": "242", "question": "Which of the following captions best describes this image?\nOptions: A: A group of people sitting around a campfire, B: A person kayaking on a lake, C: A family having a picnic in a park, D: A person hiking on a mountain trail", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/242.jpg'}", "output": ""}
{"index": "243", "question": "Which of the following captions best describes this image?\nOptions: A: A group of people playing soccer in a field, B: A woman walking her dog on a beach, C: A man riding a bicycle on a mountain trail, D: A child playing with a ball in a park", "answer": "A", "category": "coarse perception", "l2_category": "image scene and topic", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/243.jpg'}", "output": ""}
{"index": "244", "question": "Which category does this image belong to?\nOptions: A: medical CT image, B: 8-bit, C: digital art, D: photo", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/244.jpg'}", "output": ""}
{"index": "245", "question": "Which category does this image belong to?\nOptions: A: oil painting, B: sketch, C: digital art, D: photo", "answer": "C", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/245.jpg'}", "output": "c"}
{"index": "246", "question": "what style is depicted in this image?\nOptions: A: impressionism, B: post-Impressionism, C: modernism, D: dadaism", "answer": "D", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/246.jpg'}", "output": "d"}
{"index": "247", "question": "Which category does this image belong to?\nOptions: A: MRI image, B: icon, C: microscopic image, D: abstract painting", "answer": "A", "category": "coarse perception", "l2_category": "image style & quality", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/247.jpg'}", "output": ""}
{"index": "248", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Anxious, C: Happy, D: Angry", "answer": "B", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/248.jpg'}", "output": "b"}
{"index": "249", "question": "Which mood does this image convey?\nOptions: A: Sad, B: Cozy, C: Happy, D: Angry", "answer": "A", "category": "coarse perception", "l2_category": "image emotion", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/249.jpg'}", "output": ""}
{"index": "250", "question": "What type of family is shown in the image?\nOptions: A: A family of all women, B: A family of mixed genders, C: A family of all men, D: A family of only children", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/250.jpg'}", "output": "b"}
{"index": "251", "question": "How many people are playing music in the image?\nOptions: A: One, B: Two, C: Three, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/251.jpg'}", "output": "d"}
{"index": "252", "question": "What is the color of the sheep in the foreground?\nOptions: A: Black, B: Brown, C: White, D: Gray", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/252.jpg'}", "output": "c"}
{"index": "253", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the sum of 2002, 2003 and 2004?\nChoices:\n(A) 40.7\n(B) 74\n(C) 70.4\n(D) 70.0", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/253.jpg'}", "output": "d"}
{"index": "254", "question": "How many windows are in the living room?\nOptions: A: One, B: Four, C: Three, D: Two", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/254.jpg'}", "output": "d"}
{"index": "255", "question": "Which image shows the highest contrast?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/255.jpg'}", "output": ""}
{"index": "256", "question": "Which scene category matches this image the best?\nOptions: A: forest_path, B: museum/indoor, C: storage_room, D: alley", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/256.jpg'}", "output": "c"}
{"index": "257", "question": "What is the position of the catcher relative to the home plate?\nOptions: A: The catcher is to the left of the home plate., B: The catcher is to the right of the home plate., C: The catcher is behind the home plate., D: The catcher is in front of the home plate.", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/257.jpg'}", "output": "d"}
{"index": "258", "question": "What color are the walls in the bathroom?\nOptions: A: White, B: Blue, C: Beige, D: Tan", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/258.jpg'}", "output": "c"}
{"index": "259", "question": "In the given image, in which direction is the pier facing?\nOptions: A: North, B: South, C: East, D: West", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/259.jpg'}", "output": "d"}
{"index": "260", "question": "What color is the predominant color of the helmets worn by the football players in the image?\nOptions: A: White, B: Gold, C: Black, D: Blue", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/260.jpg'}", "output": "c"}
{"index": "261", "question": "What is the number of pillows on the bed in the image?\nOptions: A: Two, B: Six, C: Five, D: Four", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/261.jpg'}", "output": "d"}
{"index": "262", "question": "What type of lighting fixtures are present in the bedroom?\nOptions: A: A chandelier and ceiling fan, B: Wall sconces and a ceiling fan, C: A table lamp and floor lamp, D: Pendant lights and a ceiling fan", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/262.jpg'}", "output": "d"}
{"index": "263", "question": "How many boats are in the image?\nOptions: A: Two, B: One, C: Three, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/263.jpg'}", "output": "d"}
{"index": "264", "question": "Which woman has dark hair in the image?\nOptions: A: Both women, B: The woman standing farther from the teddy bear, C: The woman standing closer to the teddy bear, D: Neither woman", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/264.jpg'}", "output": "c"}
{"index": "265", "question": "How many gloves can be seen in this image?\nOptions: A: One, B: Two, C: Three, D: Four", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/265.jpg'}", "output": "b"}
{"index": "266", "question": "How many people are playing guitar in the image?\nOptions: A: One, B: Two, C: Three, D: Four", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/266.jpg'}", "output": "b"}
{"index": "267", "question": "How many chairs are there on the balcony in the image?\nOptions: A: 1, B: 3, C: 2, D: 4", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/267.jpg'}", "output": "c"}
{"index": "268", "question": "How many tree stumps are there in the image?\nOptions: A: nan, B: Two, C: One, D: Cannot be determined", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/268.jpg'}", "output": "c"}
{"index": "269", "question": "What is the subject of the image?\nOptions: A: Green exit sign with an arrow pointing to the right, B: Sign of a toilet on the roof of a building, C: Green exit sign with a directional arrow, D: Green exit sign with an arrow pointing to the left", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/269.jpg'}", "output": "c"}
{"index": "270", "question": "What is the color of the roof of the building in the image?\nOptions: A: Brown, B: White, C: Tan, D: Gray", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/270.jpg'}", "output": "d"}
{"index": "271", "question": "What is the color of the shirt worn by the baseball player standing on the right side of the image?\nOptions: A: Blue, B: White, C: Black, D: Gray", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/271.jpg'}", "output": "d"}
{"index": "272", "question": "What is the primary surface of the playing field in the image?\nOptions: A: Asphalt, B: Sand, C: Artificial turf, D: Grass", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/272.jpg'}", "output": "d"}
{"index": "273", "question": "How many dressers are visible in the image?\nOptions: A: nan, B: Three, C: Two, D: One", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/273.jpg'}", "output": "c"}
{"index": "274", "question": "How many people are seen playing music in the image?\nOptions: A: 1, B: 2, C: 3, D: 4", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/274.jpg'}", "output": "d"}
{"index": "275", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the different between the highest unemployment rate and the lowest?\nChoices:\n(A) 50\n(B) 105.3\n(C) 10.53\n(D) 10", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/275.jpg'}", "output": "b"}
{"index": "276", "question": "What is the dominant color in this image?\nOptions: A: Red, B: Brown, C: Black, D: White", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/276.jpg'}", "output": "c"}
{"index": "277", "question": "How many vases are in the image?\nOptions: A: 0, B: 2, C: 1, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/277.jpg'}", "output": "c"}
{"index": "278", "question": "How many people are visible in the image?\nOptions: A: One, B: Four, C: Three, D: Two", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/278.jpg'}", "output": "d"}
{"index": "279", "question": "Which object is closest to the center of the image?\nOptions: A: Person, B: Guitar, C: Drum, D: Microphone", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/279.jpg'}", "output": "b"}
{"index": "280", "question": "What is the dominant color of the person's shirt in the image?\nOptions: A: Beige, B: Gray, C: White, D: Black", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/280.jpg'}", "output": "c"}
{"index": "281", "question": "What shape is the ottoman in the living room?\nOptions: A: Square, B: Round, C: Rectangular, D: Oval", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/281.jpg'}", "output": "b"}
{"index": "282", "question": "What color is the awning in front of the store in the image?\nOptions: A: White, B: Black, C: Brown, D: Gray", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/282.jpg'}", "output": "c"}
{"index": "283", "question": "How many women are present in the image?\nOptions: A: 0, B: 1, C: 2, D: 3", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/283.jpg'}", "output": "b"}
{"index": "284", "question": "What is the position of the red rug in the living room?\nOptions: A: Under the couch, B: In front of the sliding glass door, C: Under the table, D: In the corner of the room", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/284.jpg'}", "output": ""}
{"index": "285", "question": "Where is the seated person visible in the image?\nOptions: A: On the fence, B: In the field, C: Outside the field, D: Behind the bench", "answer": "C", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/285.jpg'}", "output": "d"}
{"index": "286", "question": "What is the position of the sheep's legs in the image?\nOptions: A: At the top, B: On the right side, C: On the left side, D: At the bottom", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/286.jpg'}", "output": "b"}
{"index": "287", "question": "What is the color of the sandals in the image?\nOptions: A: Black, B: Dark brown, C: Gray, D: Light brown", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/287.jpg'}", "output": "b"}
{"index": "288", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between the leftmost and the center person? (Unit: years)\nChoices:\n(A) 0\n(B) 1\n(C) 2\n(D) 4", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/288.jpg'}", "output": "b"}
{"index": "289", "question": "How many people are performing on the stage?\nOptions: A: Three, B: Two, C: One, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/289.jpg'}", "output": ""}
{"index": "290", "question": "What is the dominant color of the ocean and sky in the image?\nOptions: A: Bright blue, B: Blue-gray, C: Gray, D: Green", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/290.jpg'}", "output": ""}
{"index": "291", "question": "Which object in the image is described as having a black top?\nOptions: A: Post, B: Building, C: Boat, D: Clock", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/291.jpg'}", "output": ""}
{"index": "292", "question": "How many objects can be seen in the image that are green?\nOptions: A: One, B: Four, C: Three, D: Two", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/292.jpg'}", "output": "d"}
{"index": "293", "question": "How many tall buildings are there in the image?\nOptions: A: Two, B: One, C: Three, D: Four", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/293.jpg'}", "output": "b"}
{"index": "294", "question": "What is located in the top right corner of the image?\nOptions: A: A person's head, B: A baseball player, C: A scoreboard, D: A soccer player", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/294.jpg'}", "output": "c"}
{"index": "295", "question": "What is the color of the woman's hair?\nOptions: A: Brown, B: Black, C: Blond, D: Red", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/295.jpg'}", "output": ""}
{"index": "296", "question": "How many people are present in the image?\nOptions: A: One, B: Three, C: Two, D: Four", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/296.jpg'}", "output": "b"}
{"index": "297", "question": "How many bananas are there in the image?\nOptions: A: 3, B: 6, C: 4, D: 5", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/297.jpg'}", "output": ""}
{"index": "298", "question": "How many mirrors are visible in the image?\nOptions: A: 0, B: 2, C: 1, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/298.jpg'}", "output": "b"}
{"index": "299", "question": "How would you describe the color of the sand in the image?\nOptions: A: Dark brown, B: White, C: Light gray, D: Golden", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/299.jpg'}", "output": "d"}
{"index": "300", "question": "How many people are visible in this picture?\nOptions: A: three, B: six, C: seven, D: eight", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/300.jpg'}", "output": "b"}
{"index": "301", "question": "How many chairs are present in the room?\nOptions: A: 2, B: 4, C: 3, D: 5", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/301.jpg'}", "output": "c"}
{"index": "302", "question": "Which object is on the left side of the image?\nOptions: A: Boat, B: Tree, C: Building, D: Water", "answer": "D", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/302.jpg'}", "output": "b"}
{"index": "303", "question": "How many white jugs are on the nightstand?\nOptions: A: 0, B: 3, C: 2, D: 1", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/303.jpg'}", "output": "d"}
{"index": "304", "question": "What bird species are shown in the image?\nOptions: A: White pelicans, B: White storks, C: Black swans, D: Snowy egrets", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/304.jpg'}", "output": "d"}
{"index": "305", "question": "How many people are present in the image?\nOptions: A: 1, B: 3, C: 2, D: 4", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/305.jpg'}", "output": "c"}
{"index": "306", "question": "How many archways are there in the image?\nOptions: A: One, B: Four, C: Three, D: Two", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/306.jpg'}", "output": "d"}
{"index": "307", "question": "How many toys are present in the image?\nOptions: A: 1, B: 3, C: 2, D: 4", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/307.jpg'}", "output": "b"}
{"index": "308", "question": "How many horses are in the image?\nOptions: A: 1, B: 2, C: 4, D: 3", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/308.jpg'}", "output": "b"}
{"index": "309", "question": "How many people are visible in the image?\nOptions: A: Two, B: One, C: Three, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/309.jpg'}", "output": ""}
{"index": "310", "question": "How many people are present in the image?\nOptions: A: 1, B: 2, C: 4, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/310.jpg'}", "output": "b"}
{"index": "311", "question": "What is the shape of the orange fruit on the table in the kitchen?\nOptions: A: Round, B: Oval, C: Square, D: Triangle", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/311.jpg'}", "output": ""}
{"index": "312", "question": "What is the color of the couch in the image?\nOptions: A: Beige, B: Brown, C: Tan, D: Gray", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/312.jpg'}", "output": "c"}
{"index": "313", "question": "How many buildings are present in the image?\nOptions: A: 1, B: 3, C: 2, D: 4", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/313.jpg'}", "output": "b"}
{"index": "314", "question": "Which of these cities is marked on the map?\nOptions: A: New York City, B: Washington, D.C., C: Philadelphia, D: Boston", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/314.jpg'}", "output": "c"}
{"index": "315", "question": "What is the predominant color in the image?\nOptions: A: Brown, B: White, C: Gray, D: Black", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/315.jpg'}", "output": "d"}
{"index": "316", "question": "How many people are walking down the snowy path?\nOptions: A: 1, B: 3, C: 2, D: 4", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/316.jpg'}", "output": "b"}
{"index": "317", "question": "Where is the Christmas tree located in the image?\nOptions: A: It is on the left-hand side of the image, B: It is on the right-hand side of the image, C: It is in the center of the image, D: It is not in the image", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/317.jpg'}", "output": "c"}
{"index": "318", "question": "How many pairs of shoes can be seen in the image?\nOptions: A: One, B: Three, C: Two, D: Cannot be determined", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/318.jpg'}", "output": "c"}
{"index": "319", "question": "What is the weather like in the image?\nOptions: A: It is sunny, B: There is snow on the ground, C: It is raining, D: There are clouds in the sky", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/319.jpg'}", "output": ""}
{"index": "320", "question": "Is the tree in front of the house with leaves or without leaves?\nOptions: A: With leaves, B: Multiple trees are visible, making it unclear which one is being referred to, C: Can't infer from the given information, D: Without leaves", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/320.jpg'}", "output": "d"}
{"index": "321", "question": "How many jewelry items are present in the image?\nOptions: A: 1, B: 3, C: 2, D: 4", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/321.jpg'}", "output": "c"}
{"index": "322", "question": "How many people are standing on the stage in the image?\nOptions: A: One, B: More than three, C: Three, D: Two", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/322.jpg'}", "output": "b"}
{"index": "323", "question": "What is the color of the couch in the living room?\nOptions: A: Beige, B: Grey, C: Dark Brown, D: White", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/323.jpg'}", "output": "c"}
{"index": "324", "question": "How many apples are there in the image? And how many bananas are there?\nOptions: A: 3 apples and 1 bananas, B: 3 apples and 2 bananas, C: 1 apples and 1 bananas, D: 2 apples and 1 bananas", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/324.jpg'}", "output": "c"}
{"index": "325", "question": "Which corner doesn't have any fruits?\nOptions: A: top-right, B: top-left, C: bottom-left, D: bottom-right", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/325.jpg'}", "output": "c"}
{"index": "326", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: whats the lowest number yard line that you can see?\nChoices:\n(A) 0\n(B) 10\n(C) 20\n(D) 30", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/326.jpg'}", "output": "d"}
{"index": "327", "question": "What is the color of the ears on the dessert item located in the bottom right of the image?\nOptions: A: Red, B: Brown, C: Black, D: White", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/327.jpg'}", "output": "c"}
{"index": "328", "question": "What is the color of the watch worn by a person in the image?\nOptions: A: White, B: Black, C: Silver, D: Gold", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/328.jpg'}", "output": "d"}
{"index": "329", "question": "What color is the horse in the image?\nOptions: A: Black, B: Brown, C: Grey, D: White", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/329.jpg'}", "output": "d"}
{"index": "330", "question": "How many persons are playing guitar in the concert?\nOptions: A: 1, B: 2, C: 3, D: 4", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/330.jpg'}", "output": "b"}
{"index": "331", "question": "How many people are present in the image?\nOptions: A: One, B: Two, C: Three, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/331.jpg'}", "output": "c"}
{"index": "332", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the accuracy of the algorithm with lowest accuracy?\nChoices:\n(A) 1\n(B) 0.8\n(C) 0.6\n(D) 0.4", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/332.jpg'}", "output": ""}
{"index": "333", "question": "What color is the shirt of the man playing the guitar?\nOptions: A: Blue, B: Brown, C: Red, D: Black", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/333.jpg'}", "output": "d"}
{"index": "334", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the value of the largest individual bar in the whole chart?\nChoices:\n(A) 8\n(B) 5\n(C) 6\n(D) 7", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/334.jpg'}", "output": ""}
{"index": "335", "question": "How many clear glasses are on the table in the image?\nOptions: A: Three, B: Six, C: Five, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/335.jpg'}", "output": "d"}
{"index": "336", "question": "How many people are holding the flag in the street?\nOptions: A: 3, B: 2, C: 1, D: Cannot be determined", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/336.jpg'}", "output": ""}
{"index": "337", "question": "What is the color of the ribbon in one of the gifts?\nOptions: A: White, B: Blue, C: Gold, D: Silver", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/337.jpg'}", "output": ""}
{"index": "338", "question": "How many guitars are visible in the image?\nOptions: A: 2, B: 1, C: 3, D: 4", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/338.jpg'}", "output": "c"}
{"index": "339", "question": "What is the shape of the window on the building in the image?\nOptions: A: Square, B: Rectangle, C: Circle, D: Triangle", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/339.jpg'}", "output": "c"}
{"index": "340", "question": "What color is the ribbon that the man on the right is holding?\nOptions: A: Red, B: Green, C: Yellow, D: Blue", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/340.jpg'}", "output": "b"}
{"index": "341", "question": "How many women are sitting at the table in the image?\nOptions: A: One, B: Two, C: Four, D: Three", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/341.jpg'}", "output": "d"}
{"index": "342", "question": "What is the color of the net behind the players in the image?\nOptions: A: It is not visible in the image., B: Blue, C: Red, D: White", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/342.jpg'}", "output": "d"}
{"index": "343", "question": "Where is the man wearing a black backpack?\nOptions: A: In the foreground, B: In the background, C: In the middle of the image, D: Cannot be determined", "answer": "C", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/343.jpg'}", "output": "c"}
{"index": "344", "question": "What is the color of the ball being used in the game of golf?\nOptions: A: Red, B: Yellow, C: Green, D: Blue", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/344.jpg'}", "output": "d"}
{"index": "345", "question": "Does the image contain any person wearing green clothes?\nOptions: A: Yes, B: Cannot determine from the given information, C: No, D: None of the above", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/345.jpg'}", "output": ""}
{"index": "346", "question": "What color are the flowers in the room?\nOptions: A: White, B: Yellow, C: Red, D: Pink", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/346.jpg'}", "output": "b"}
{"index": "347", "question": "What is the material of the boots that the woman is wearing in the image?\nOptions: A: Suede, B: Leather, C: Canvas, D: Synthetic", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/347.jpg'}", "output": ""}
{"index": "348", "question": "What type of ball is being held by the player?\nOptions: A: A tennis ball, B: A basketball, C: A volleyball, D: A soccer ball", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/348.jpg'}", "output": "d"}
{"index": "349", "question": "How many paintings are in the image?\nOptions: A: 0, B: 2, C: 1, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/349.jpg'}", "output": "b"}
{"index": "350", "question": "What is the color of the writing in the image?\nOptions: A: White, B: Black, C: Red, D: Blue", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/350.jpg'}", "output": ""}
{"index": "351", "question": "Which can be the associated text with this image posted on twitter\nOptions: A: Morning: Memeland Evening: Jay Chou 7 sold out nights in #hongkong #JayChou, B: We will be streaming our Mayday [ Live In the Sky ] online concert tomorrow night: http://bit.ly/YTBinMusic . We go on at 20:00 (GMT+8) May 31st. See you online then., C: my little airport , D: Run to Victoria Harbor at night", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/351.jpg'}", "output": ""}
{"index": "352", "question": "How many people are on stage?\nOptions: A: 2, B: 3, C: 4, D: 5", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/352.jpg'}", "output": "c"}
{"index": "353", "question": "Does the man have any facial hair?\nOptions: A: No, B: Yes, a mustache, C: Yes, a full beard, D: Yes, slight stubble", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/353.jpg'}", "output": "d"}
{"index": "354", "question": "What piece of furniture is in the foreground of the picture?\nOptions: A: A couch, B: A chair, C: A lamp, D: A table", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/354.jpg'}", "output": ""}
{"index": "355", "question": "What is the shape of the object with an angel face and wings in the image?\nOptions: A: Egg-shaped, B: Square, C: Round, D: Star-shaped", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/355.jpg'}", "output": "c"}
{"index": "356", "question": "What is the most prominent color of the countertop?\nOptions: A: Gray, B: White, C: Black, D: Brown", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/356.jpg'}", "output": "b"}
{"index": "357", "question": "How many pillows can be seen in the bedroom?\nOptions: A: 5, B: 4, C: 3, D: 6", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/357.jpg'}", "output": "b"}
{"index": "358", "question": "Which term matches the picture?\nOptions: A: filtration, B: centrifugation, C: nan, D: nan", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/358.jpg'}", "output": ""}
{"index": "359", "question": "Roughly how much of the picture is occupied by the two people on the bench in the picture?\nOptions: A: more than 50%, B: less than 30%, C: 0.8, D: more than 60%", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/359.jpg'}", "output": "b"}
{"index": "360", "question": "here is the woman?\nOptions: A: The woman is on the bottom right, B: The woman is on the top right, C: The woman is in the center, D: The woman is on the top left", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/360.jpg'}", "output": "c"}
{"index": "361", "question": "How many dogs can be seen in the image?\nOptions: A: 3, B: 2, C: 1, D: 4", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/361.jpg'}", "output": "b"}
{"index": "362", "question": "Who is the person in this image?\nOptions: A: Kobe Bryant, B: Jing Wu, C: Morgan Freeman, D: Jay Chou", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/362.jpg'}", "output": "d"}
{"index": "363", "question": "What color is the text in the image?\nOptions: A: White, B: Blue, C: Black, D: Grey", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/363.jpg'}", "output": ""}
{"index": "364", "question": "Name the topmost opening of a volcano\nOptions: A: Camino Proncipale, B: Camino volcanico, C: Cratere volcanico, D: Cratere Proncipale", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/364.jpg'}", "output": ""}
{"index": "365", "question": "How many apples are there in the image? And how many bananas are there?\nOptions: A: 4 apples and 2 bananas, B: 3 apples and 3 banana, C: 2 apples and 4 bananas, D: 4 apples and 1 bananas", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/365.jpg'}", "output": "b"}
{"index": "366", "question": "Which corner are the red bananas?\nOptions: A: top-right, B: top-left, C: bottom-left, D: bottom-right", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/366.jpg'}", "output": ""}
{"index": "367", "question": "Based on the image, how many soccer players are on the field?\nOptions: A: 1, B: 2, C: 4, D: 3", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/367.jpg'}", "output": "d"}
{"index": "368", "question": "Where is it located?\nOptions: A: Chengdu, B: Canton, C: Beijing, D: Xi'an", "answer": "D", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/368.jpg'}", "output": "d"}
{"index": "369", "question": "What color is the microphone on stage?\nOptions: A: Pink, B: Silver, C: Red, D: Black", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/369.jpg'}", "output": "d"}
{"index": "370", "question": "What is the color of the hat that the person in the image is wearing?\nOptions: A: Blue, B: Purple, C: Yellow, D: Pink", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/370.jpg'}", "output": "b"}
{"index": "371", "question": "In the given image, where is the chair made out of tree trunks and stumps located?\nOptions: A: On the left side of the image, B: Close to the right side of the image, C: In the center of the image, D: Behind the tree in the image", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/371.jpg'}", "output": ""}
{"index": "372", "question": "What is the main item on the counter in the image?\nOptions: A: A cup of ice cream on a counter with a cone on top, B: Two ice cream cones sitting next to each other, C: A coffee cup with whipped cream on top, D: A cup of ice cream with different flavors", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/372.jpg'}", "output": "d"}
{"index": "373", "question": "What is the color of the rug in the image?\nOptions: A: Brown, B: White, C: Grey, D: Black", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/373.jpg'}", "output": ""}
{"index": "374", "question": "What is the color of the ceiling in the image?\nOptions: A: Brown, B: Black, C: White, D: Gray", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/374.jpg'}", "output": "d"}
{"index": "375", "question": "How many people are there in the image?\nOptions: A: 0, B: 1, C: 2, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/375.jpg'}", "output": "b"}
{"index": "376", "question": "What is the color of the earring worn by the bride?\nOptions: A: Black, B: Gold, C: White, D: Silver", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/376.jpg'}", "output": "b"}
{"index": "377", "question": "Is there a person walking in the image?\nOptions: A: The image is too blurry to tell, B: No, C: There is more than one person walking, D: Yes", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/377.jpg'}", "output": ""}
{"index": "378", "question": "What is the color of the letter in the red circle?\nOptions: A: Black, B: White, C: Red, D: Yellow", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/378.jpg'}", "output": "b"}
{"index": "379", "question": "What color is the softball?\nOptions: A: Blue, B: Red, C: Yellow, D: Green", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/379.jpg'}", "output": "c"}
{"index": "380", "question": "What is the color of the eye in the mask in the image?\nOptions: A: Blue and green, B: Purple and pink, C: Brown and black, D: Yellow and black", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/380.jpg'}", "output": "d"}
{"index": "381", "question": "Who is the person in this image?\nOptions: A: Elon Musk, B: Xiang Liu, C: Jay Chou, D: Ming Yao", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/381.jpg'}", "output": "c"}
{"index": "382", "question": "How many pillows are on the bed closest to the window?\nOptions: A: 0, B: 1, C: 3, D: 2", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/382.jpg'}", "output": "d"}
{"index": "383", "question": "What color is the writing on the sign in the image?\nOptions: A: White, B: Green, C: Red, D: Black", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/383.jpg'}", "output": "d"}
{"index": "384", "question": "How many people are visible in the image?\nOptions: A: One, B: Two, C: Four, D: Three", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/384.jpg'}", "output": "b"}
{"index": "385", "question": "What color is the raincoat of the man on the left?\nOptions: A: red, B: green, C: yellow, D: blue", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/385.jpg'}", "output": "c"}
{"index": "386", "question": "Which direction is the sun shining from?\nOptions: A: Right, B: Left, C: Up, D: Down", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/386.jpg'}", "output": ""}
{"index": "387", "question": "What is the girl wearing in the image?\nOptions: A: A white dress, B: An apron, C: A black sandal, D: A pink dress", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/387.jpg'}", "output": "d"}
{"index": "388", "question": "The tiny shiny cylinder has what color?\nOptions: A: red, B: cyan, C: purple, D: brown", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/388.jpg'}", "output": "d"}
{"index": "389", "question": "How many people in the image have a visible smile?\nOptions: A: Two, B: Four, C: Three, D: Five", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/389.jpg'}", "output": "c"}
{"index": "390", "question": "Where is the woman located in the picture?\nOptions: A: left, B: right, C: top, D: bottom", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/390.jpg'}", "output": "b"}
{"index": "391", "question": "How many angels have halos?\nOptions: A: Two, B: One, C: nan, D: Three", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/391.jpg'}", "output": "d"}
{"index": "392", "question": "What type of pavement is the person with the dog walking on?\nOptions: A: Concrete, B: Brick, C: Asphalt, D: Gravel", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/392.jpg'}", "output": "b"}
{"index": "393", "question": "Can you see a flag in the image?\nOptions: A: Yes, it's in the middle of the frame, B: Yes, it's close to the bottom left corner, C: No, there's no flag in the image, D: Yes, it's to the right of the frame", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/393.jpg'}", "output": "c"}
{"index": "394", "question": "What is the activity of the person in the foreground?\nOptions: A: sitting, B: running, C: playing soccer, D: watching the game", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/394.jpg'}", "output": "c"}
{"index": "395", "question": "What is the name of the place shown?\nOptions: A: Virginia, B: Michigan, C: Kentucky, D: Maryland", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/395.jpg'}", "output": ""}
{"index": "396", "question": "What is the color of the shirt worn by a man playing the guitar?\nOptions: A: Blue, B: Black, C: Red, D: White", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/396.jpg'}", "output": "d"}
{"index": "397", "question": "Where is the soccer ball located?\nOptions: A: On the ground in the bottom left corner, B: In the air in the top right corner, C: On the ground in the bottom right corner, D: In the air in the top left corner", "answer": "D", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/397.jpg'}", "output": "b"}
{"index": "398", "question": "What is the primary color of the football in the image?\nOptions: A: Maroon, B: Red, C: Brown, D: Orange", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/398.jpg'}", "output": "c"}
{"index": "399", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Look at the following schedule. Which activity begins at 11.50 A.M.?'\nChoices:\n(A) figure skating practice\n(B) private class\n(C) adult class\n(D) children's class", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/399.jpg'}", "output": "d"}
{"index": "400", "question": "What is the color of the man's tie in the image?\nOptions: A: Blue, B: White, C: Black, D: Red", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/400.jpg'}", "output": "c"}
{"index": "401", "question": "How many people are on the stage in the image?\nOptions: A: Three, B: Two, C: One, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/401.jpg'}", "output": "d"}
{"index": "402", "question": "What is the main color of the large neon sign in the image?\nOptions: A: Black, B: White, C: Pink, D: Red", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/402.jpg'}", "output": "d"}
{"index": "403", "question": "How many ties can be seen in the image?\nOptions: A: 3, B: 2, C: 1, D: 4", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/403.jpg'}", "output": "b"}
{"index": "404", "question": "How many people are present in the baseball game?\nOptions: A: 2, B: 4, C: 3, D: 5", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/404.jpg'}", "output": "c"}
{"index": "405", "question": "In the scene, where is the phone located?\nOptions: A: In the woman's hand, B: The phone is not visible, C: On the ground near the couple, D: In the man's hand", "answer": "D", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/405.jpg'}", "output": "d"}
{"index": "406", "question": "According to this picture, how old are Dennis.\nOptions: A: 38, B: 45, C: 29, D: 47", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/406.jpg'}", "output": "b"}
{"index": "407", "question": "Where is the drum located in the image?\nOptions: A: In the back, B: On the left side, C: On the right side, D: At the center", "answer": "C", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/407.jpg'}", "output": "d"}
{"index": "408", "question": "What is the main structure in the image?\nOptions: A: Roman castle, B: Modern building, C: Bridge, D: Archway", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/408.jpg'}", "output": "c"}
{"index": "409", "question": "What is the position of the grape in relation to the plate of walnuts?\nOptions: A: The grape is on top of the plate of walnuts, B: The grape is below the plate of walnuts, C: The grape is to the right of the plate of walnuts, D: The grape is to the left of the plate of walnuts", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/409.jpg'}", "output": ""}
{"index": "410", "question": "What is the relative position of the woman's hand to her body in the image?\nOptions: A: Beside her head, B: At her side, C: Raised above her head, D: Resting on her hip", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/410.jpg'}", "output": ""}
{"index": "411", "question": "where is the cat?\nOptions: A: top-right, B: top-left, C: bottom-left, D: bottom-right", "answer": "D", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/411.jpg'}", "output": "d"}
{"index": "412", "question": "how many dogs are there\nOptions: A: 3, B: 4, C: 2, D: 6", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/412.jpg'}", "output": "b"}
{"index": "413", "question": "who is this person?\nOptions: A: Victoria Beckham, B: Helen Mirren, C: Kate Winslet, D: Keira Knightley", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/413.jpg'}", "output": "c"}
{"index": "414", "question": "who is this person?\nOptions: A: Hailee Steinfeld, B: Sridevi, C: Sandra Oh, D: Deepika Padukone", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/414.jpg'}", "output": "d"}
{"index": "415", "question": "What is the primary type of food that is in the image?\nOptions: A: Cookies, B: Potatoes, C: Indian curry, D: Fried dough", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/415.jpg'}", "output": ""}
{"index": "416", "question": "What color are the trees in the background?\nOptions: A: Brown, B: Yellow, C: Red, D: Green", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/416.jpg'}", "output": "d"}
{"index": "417", "question": "What is the color of the hat in the image?\nOptions: A: Black, B: Brown, C: Beige, D: White", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/417.jpg'}", "output": "b"}
{"index": "418", "question": "How many people are on stage?\nOptions: A: 3, B: 4, C: 5, D: 6", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/418.jpg'}", "output": "b"}
{"index": "419", "question": "What is the gender of the person playing the keyboard?\nOptions: A: Multiple persons are playing the keyboard, B: Female, C: Cannot determine due to insufficient information, D: Male", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/419.jpg'}", "output": "d"}
{"index": "420", "question": "How many stages are shown in the diagram?\nOptions: A: 6, B: 7, C: 3, D: 2", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/420.jpg'}", "output": "c"}
{"index": "421", "question": "What is the infect called at A?\nOptions: A: Nymphose, B: Coque, C: Larve, D: Imago", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/421.jpg'}", "output": "c"}
{"index": "422", "question": "How many musical instruments are in the image?\nOptions: A: 1, B: 2, C: 4, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/422.jpg'}", "output": "b"}
{"index": "423", "question": "What is the position of the drum set in relation to the man playing guitar?\nOptions: A: Directly behind, B: In front but to the side, C: To the right, D: To the left", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/423.jpg'}", "output": "b"}
{"index": "424", "question": "What is the color of the word \"brick\" as it appears on the wall in the image?\nOptions: A: White, B: Gray, C: Red, D: Black", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/424.jpg'}", "output": "b"}
{"index": "425", "question": "How many older men are playing guitars in the image?\nOptions: A: 1, B: 4, C: 3, D: 2", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/425.jpg'}", "output": "d"}
{"index": "426", "question": "Where is it?\nOptions: A: Shanghai, B: Xi'an, C: Wuhan, D: Nanjing", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/426.jpg'}", "output": "b"}
{"index": "427", "question": "Where is the stage located in the image relative to the musician?\nOptions: A: Above the musician, B: To the right of the musician, C: To the left of the musician, D: Below the musician", "answer": "D", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/427.jpg'}", "output": "c"}
{"index": "428", "question": "How many bananas are there in the image?\nOptions: A: 3, B: 2, C: 4, D: 5", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/428.jpg'}", "output": "b"}
{"index": "429", "question": "where is the cat?\nOptions: A: top-right, B: top-left, C: bottom-left, D: bottom-right", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/429.jpg'}", "output": "b"}
{"index": "430", "question": "How many soccer players are present in the image?\nOptions: A: 6, B: 8, C: 10, D: 12", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/430.jpg'}", "output": "b"}
{"index": "431", "question": "What is the color of the socks detected in the attribute detection of the image?\nOptions: A: Black, B: Red, C: White, D: Striped", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/431.jpg'}", "output": "c"}
{"index": "432", "question": "What type of electronic device is on the table?\nOptions: A: Tablet, B: Laptop computer, C: Smartphone, D: Desktop computer", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/432.jpg'}", "output": "b"}
{"index": "433", "question": "How many men are visible in the image?\nOptions: A: 2, B: 1, C: 3, D: 4", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/433.jpg'}", "output": "c"}
{"index": "434", "question": "What type of flowers is the bride holding in her bouquet?\nOptions: A: Pink and purple, B: Yellow and white, C: White and pink, D: White and purple", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/434.jpg'}", "output": "c"}
{"index": "435", "question": "How many teddy bears are in the picture?\nOptions: A: Two, B: One, C: Three, D: Four", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/435.jpg'}", "output": "b"}
{"index": "436", "question": "How many chairs are visible in the image?\nOptions: A: 2, B: 3, C: 4, D: 5", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/436.jpg'}", "output": ""}
{"index": "437", "question": "How many people are on the stage during the presentation?\nOptions: A: 6, B: 4, C: 2, D: 8", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/437.jpg'}", "output": ""}
{"index": "438", "question": "What is the position of the person playing an instrument in the image?\nOptions: A: Standing on the left side of the stage, B: Standing on the right side of the stage, C: Sitting in the center of the stage, D: Standing at the back of the stage", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/438.jpg'}", "output": "b"}
{"index": "439", "question": "How many guitars are being played in the image?\nOptions: A: Two, B: One, C: Three, D: Four", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/439.jpg'}", "output": ""}
{"index": "440", "question": "What is the color of the waitress' apron?\nOptions: A: Black, B: White, C: Red, D: Green", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/440.jpg'}", "output": ""}
{"index": "441", "question": "What color is the writing in the image?\nOptions: A: Black, B: White, C: Blue, D: Red", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/441.jpg'}", "output": "b"}
{"index": "442", "question": "Which statement about the girl's hair is true?\nOptions: A: Her hair is in a ponytail, B: Her hair is blonde, C: Her hair is curly, D: Her hair is short", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/442.jpg'}", "output": "d"}
{"index": "443", "question": "Where is the guitarist in relation to the other instrumentalists?\nOptions: A: Behind them, B: In front of them, C: To the left of them, D: To the right of them", "answer": "C", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/443.jpg'}", "output": "b"}
{"index": "444", "question": "In the image, what is the position of the blanket relative to the people?\nOptions: A: Above, B: Below, C: Beside, D: On top of", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/444.jpg'}", "output": "c"}
{"index": "445", "question": "What is the color of the shorts of the soccer player wearing a red and black uniform?\nOptions: A: White and black, B: Yellow and green, C: Black and white, D: Blue and red", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/445.jpg'}", "output": "c"}
{"index": "446", "question": "How many people are visible in the black and white photo of the band on stage?\nOptions: A: 1, B: 2, C: 3, D: 4", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/446.jpg'}", "output": "d"}
{"index": "447", "question": "How many tables are there in the room?\nOptions: A: Four tables, B: Two tables, C: Three tables, D: One table", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/447.jpg'}", "output": "d"}
{"index": "448", "question": "What is the woman holding in her hand?\nOptions: A: A book, B: A camera, C: A cell phone, D: Nothing", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/448.jpg'}", "output": "b"}
{"index": "449", "question": "Where is the table in relation to the balcony?\nOptions: A: On the right side of the balcony, B: In the center of the balcony, C: On the left side of the balcony, D: Cannot be determined from the information given", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/449.jpg'}", "output": ""}
{"index": "450", "question": "How many blue balloons are in the room?\nOptions: A: Three, B: Two, C: One, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/450.jpg'}", "output": "d"}
{"index": "451", "question": "How would you describe the condition of the sky in the image?\nOptions: A: Cloudy, blue, B: Clear, blue, C: Small white clouds, D: Stormy", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/451.jpg'}", "output": "b"}
{"index": "452", "question": "Is there a microphone in the image?\nOptions: A: Cannot determine, B: No, C: Yes, D: Not enough information", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/452.jpg'}", "output": "b"}
{"index": "453", "question": "How many plants are in the image?\nOptions: A: 1, B: 2, C: 3, D: 4", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/453.jpg'}", "output": ""}
{"index": "454", "question": "What is the position of the person on stage?\nOptions: A: Standing on the left side, B: Seated in the center, C: Sitting on the floor in front, D: Leaning on the guitar on the right side", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/454.jpg'}", "output": "b"}
{"index": "455", "question": "How many colors are the eyes of the depicted animals?\nOptions: A: Two, B: One, C: Three, D: Four", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/455.jpg'}", "output": ""}
{"index": "456", "question": "What is the color of the number on the red and blue jersey in the image?\nOptions: A: Blue, B: Red, C: Orange, D: Black", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/456.jpg'}", "output": "b"}
{"index": "457", "question": "How many referees are present in the image?\nOptions: A: 0, B: 1, C: 2, D: 3", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/457.jpg'}", "output": "d"}
{"index": "458", "question": "How many trees are in the image?\nOptions: A: Zero, B: Three, C: Two, D: One", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/458.jpg'}", "output": "c"}
{"index": "459", "question": "What is the color of the lettering on the green sign in the image?\nOptions: A: Black, B: Blue, C: Yellow, D: White", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/459.jpg'}", "output": "d"}
{"index": "460", "question": "How many types of fruits are in the bowl?\nOptions: A: 4, B: 2, C: 3, D: 5", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/460.jpg'}", "output": "c"}
{"index": "461", "question": "How many ponds are clearly visible in this image?\nOptions: A: One, B: Three, C: Two, D: Four", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/461.jpg'}", "output": "c"}
{"index": "462", "question": "What is the predominant color of the drum detected in the image?\nOptions: A: Black, B: Brown, C: Red, D: Blue", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/462.jpg'}", "output": "b"}
{"index": "463", "question": "How many chairs are detected in the image?\nOptions: A: 4, B: 3, C: 2, D: 5", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/463.jpg'}", "output": "c"}
{"index": "464", "question": "Where is the snail located in the image?\nOptions: A: On a tree trunk, B: On top of a mushroom, C: On a log, D: On a rock", "answer": "C", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/464.jpg'}", "output": ""}
{"index": "465", "question": "How many drawers can be seen in the white kitchen?\nOptions: A: Three, B: Two, C: One, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/465.jpg'}", "output": ""}
{"index": "466", "question": "What color is the sky in the image?\nOptions: A: Blue, B: Yellow, C: Gray, D: Cannot be determined", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/466.jpg'}", "output": "c"}
{"index": "467", "question": "How many windows are in the room?\nOptions: A: 1, B: 2, C: 4, D: 3", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/467.jpg'}", "output": "d"}
{"index": "468", "question": "How many wooden objects are in the image?\nOptions: A: 1, B: 2, C: 3, D: 4", "answer": "A", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/468.jpg'}", "output": "b"}
{"index": "469", "question": "What color is the building in the foreground?\nOptions: A: Blue, B: Grey, C: White, D: Black", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/469.jpg'}", "output": "b"}
{"index": "470", "question": "How many musical instruments can be seen in the image?\nOptions: A: One, B: Four, C: Three, D: Two", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/470.jpg'}", "output": "d"}
{"index": "471", "question": "How many drawers can be seen in the image?\nOptions: A: One, B: Three, C: Two, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/471.jpg'}", "output": "b"}
{"index": "472", "question": "What is the color scheme of the costumes worn by the men in the parade?\nOptions: A: Yellow, red, and gold, B: Red, blue, and black, C: Green, orange, and white, D: Black, white, and gray", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/472.jpg'}", "output": "b"}
{"index": "473", "question": "How many people are present in the image?\nOptions: A: Four, B: Ten, C: Eight, D: Six", "answer": "C", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/473.jpg'}", "output": "d"}
{"index": "474", "question": "What is the most prominent color of the jacket worn by the man with the laptop?\nOptions: A: Blue, B: Black, C: Gray, D: Red", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/474.jpg'}", "output": "b"}
{"index": "475", "question": "What is the color of the ball closest to the man?\nOptions: A: White, B: Red, C: Blue, D: Brown", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/475.jpg'}", "output": ""}
{"index": "476", "question": "How many balls are on the table?\nOptions: A: 6, B: 4, C: 5, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/476.jpg'}", "output": ""}
{"index": "477", "question": "How many people are on stage in the image?\nOptions: A: One, B: Four, C: Three, D: Two", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/477.jpg'}", "output": "b"}
{"index": "478", "question": "How many street lights are visible in the image?\nOptions: A: 1, B: 2, C: 4, D: 3", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/478.jpg'}", "output": "b"}
{"index": "479", "question": "Who is wearing black pants in the image?\nOptions: A: The man on stage, B: The person in the background, C: The woman playing an instrument, D: The person walking in the room", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/479.jpg'}", "output": "c"}
{"index": "480", "question": "What is the main color of the shirt the woman is wearing?\nOptions: A: White, B: Blue, C: Pink, D: Black", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/480.jpg'}", "output": "b"}
{"index": "481", "question": "Which object is located in the center of the image?\nOptions: A: The wooden doll, B: The cartoon family, C: The porcelain figurine, D: The film character", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/481.jpg'}", "output": "b"}
{"index": "482", "question": "How many people are playing instruments in the image?\nOptions: A: One, B: Three, C: Two, D: Four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/482.jpg'}", "output": "d"}
{"index": "483", "question": "What is the primary color of the shoes worn by the player closest to the camera?\nOptions: A: Yellow, B: Black, C: Gray, D: Orange", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/483.jpg'}", "output": "d"}
{"index": "484", "question": "Where is the pathway located in the image?\nOptions: A: On the left side of the image, B: On the right side of the image, C: In the center of the image, D: Cannot be determined from the information provided", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/484.jpg'}", "output": "b"}
{"index": "485", "question": "What color is the microphone in the image?\nOptions: A: Silver, B: White, C: Black, D: Red", "answer": "C", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/485.jpg'}", "output": "b"}
{"index": "486", "question": "How many tires can be seen in the image?\nOptions: A: 4, B: 3, C: 5, D: 6", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/486.jpg'}", "output": "b"}
{"index": "487", "question": "Is there a bird in the image?\nOptions: A: Yes, B: Both A and B, C: Cannot determine, D: No", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/487.jpg'}", "output": ""}
{"index": "488", "question": "What is the position of the dog in the image?\nOptions: A: In the center, B: On the left side, C: On the right side, D: At the top", "answer": "B", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/488.jpg'}", "output": "c"}
{"index": "489", "question": "How many people are visible in the image?\nOptions: A: Four, B: Two, C: Three, D: One", "answer": "B", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/489.jpg'}", "output": "d"}
{"index": "490", "question": "Where is the giraffe located in the picture?\nOptions: A: right, B: top, C: bottom, D: left", "answer": "D", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/490.jpg'}", "output": "d"}
{"index": "491", "question": "Where is the person in the Alice in Wonderland costume located?\nOptions: A: In the middle of the image, B: Close to the right side of the image, C: Close to the left side of the image, D: At the bottom of the image", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/491.jpg'}", "output": "d"}
{"index": "492", "question": "What is the primary color of the glove held by the player in the foreground?\nOptions: A: Black, B: Orange, C: Brown, D: Yellow", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/492.jpg'}", "output": "c"}
{"index": "493", "question": "Where is it located?\nOptions: A: Xi'an, B: Shanghai, C: Beijing, D: Nanjing", "answer": "A", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/493.jpg'}", "output": ""}
{"index": "494", "question": "In the picture, which direction is the teddy bear facing?\nOptions: A: left, B: right, C: upward, D: downward", "answer": "C", "category": "fine-grained perception", "l2_category": "localization", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/494.jpg'}", "output": "b"}
{"index": "495", "question": "How many computer monitors are in this picture?\nOptions: A: eight, B: one, C: three, D: four", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/495.jpg'}", "output": "d"}
{"index": "496", "question": "What color is the hair of the woman in the image?\nOptions: A: Black, B: Brown, C: Red, D: Blonde", "answer": "A", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/496.jpg'}", "output": ""}
{"index": "497", "question": "How many soccer players are on the field?\nOptions: A: 5, B: 11, C: 9, D: 7", "answer": "D", "category": "fine-grained perception", "l2_category": "object counting", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/497.jpg'}", "output": "d"}
{"index": "498", "question": "What color are the man's shoes?\nOptions: A: Gray, B: Brown, C: Blue, D: Black", "answer": "D", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/498.jpg'}", "output": "d"}
{"index": "499", "question": "How can you best describe the pillow on the chair?\nOptions: A: Plain white, B: Colorful and striped, C: Solid blue, D: Floral pattern", "answer": "B", "category": "fine-grained perception", "l2_category": "recognition", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/499.jpg'}", "output": "c"}
{"index": "500", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between these two people in image? (Unit: years)\nChoices:\n(A) 4\n(B) 2\n(C) 3\n(D) 1", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/500.jpg'}", "output": "d"}
{"index": "501", "question": "What is the approximate time of day in the image?\nOptions: A: Morning, B: Evening, C: Noon, D: Night", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/501.jpg'}", "output": "b"}
{"index": "502", "question": "What is the relation between the door and the brown panel?\nOptions: A: The panel is next to the door, B: The panel is above the door, C: They are the same object, D: The panel is behind the door", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/502.jpg'}", "output": ""}
{"index": "503", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between these two people in image? (Unit: years)\nChoices:\n(A) 10\n(B) 20\n(C) 16\n(D) 32", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/503.jpg'}", "output": "d"}
{"index": "504", "question": "What is the relation between the arrow and the curve sign?\nOptions: A: The arrow is pointing away from the curve sign, B: The arrow is pointing to the curve sign, C: The arrow and the curve sign are unrelated, D: The arrow and the curve sign are overlapping", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/504.jpg'}", "output": "b"}
{"index": "505", "question": "Which of the following statements match the image?\nOptions: A: A green pentagon is above a red shape., B: A red ellipse is above a green pentagon., C: A yellow shape is below a red pentagon., D: A pentagon is below a pentagon.", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/505.jpg'}", "output": ""}
{"index": "506", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between these two people in image? (Unit: years)\nChoices:\n(A) 4\n(B) 5\n(C) 6\n(D) 7", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/506.jpg'}", "output": ""}
{"index": "507", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between these two people in image? (Unit: years)\nChoices:\n(A) 4\n(B) 5\n(C) 6\n(D) 2", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/507.jpg'}", "output": "d"}
{"index": "508", "question": "Are the two hoops in the picture the same size?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/508.jpg'}", "output": "b"}
{"index": "509", "question": "In this comparison picture, are the left and right modules the same shape?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/509.jpg'}", "output": "b"}
{"index": "510", "question": "What is the dominant color of the clothing worn by the models in the image?\nOptions: A: Maroon, B: White, C: Black, D: Red", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/510.jpg'}", "output": ""}
{"index": "511", "question": "Which of the following statements match the image?\nOptions: A: A yellow triangle is below a red rectangle., B: A cross is above a cyan shape., C: A rectangle is above a cyan shape., D: A cyan rectangle is below a red shape.", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/511.jpg'}", "output": "d"}
{"index": "512", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between these two people in image? (Unit: years)\nChoices:\n(A) 0\n(B) 2\n(C) 5\n(D) 9", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/512.jpg'}", "output": "b"}
{"index": "513", "question": "What direction is Serbia in the Mediterranean Sea?\nOptions: A: east, B: south, C: west, D: north", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/513.jpg'}", "output": "c"}
{"index": "514", "question": "What direction is Canada in the Atlantic Ocean?\nOptions: A: east, B: south, C: west, D: north", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/514.jpg'}", "output": "d"}
{"index": "515", "question": "Which object is taller, the tree or the building?\nOptions: A: Cannot be determined, B: The building, C: They are the same height, D: The tree", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/515.jpg'}", "output": "b"}
{"index": "516", "question": "What is the position of the woman in relation to the man?\nOptions: A: To his left, B: Behind him, C: In front of him, D: To his right", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/516.jpg'}", "output": "c"}
{"index": "517", "question": "How many lamps are in the bedroom?\nOptions: A: One, B: Two, C: Three, D: Four", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/517.jpg'}", "output": "b"}
{"index": "518", "question": "What is the relative position of the church and the building in this image?\nOptions: A: The church and the building are on the opposite sides, B: The church is on the right and the building is on the left, C: The church is in the center and the building is on the side, D: The church is on the left and the building is on the right", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/518.jpg'}", "output": "c"}
{"index": "519", "question": "What is located outside the windows in the hallway?\nOptions: A: Another house, B: A garden, C: A cityscape, D: A grassy field", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/519.jpg'}", "output": "d"}
{"index": "520", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/520.jpg'}", "output": "c"}
{"index": "521", "question": "What is the relative position between the basketball player and the basket in the image?\nOptions: A: The basketball player is to the left of the basket, B: The basketball player is above the basket, C: The basketball player is beneath the basket, D: The basketball player is to the right of the basket", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/521.jpg'}", "output": "b"}
{"index": "522", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/522.jpg'}", "output": "b"}
{"index": "523", "question": "What is the material of the shower in the bathroom?\nOptions: A: Metal, B: Glass, C: Plastic, D: Wood", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/523.jpg'}", "output": "d"}
{"index": "524", "question": "What is the dominant material of the cabinets in the image?\nOptions: A: Ceramic, B: Metal, C: Wood, D: Glass", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/524.jpg'}", "output": "c"}
{"index": "525", "question": "Which of the following are on the back of a vehicle being lifted by a crane in the image?\nOptions: A: Tires, B: People, C: License plates, D: Motorcycles", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/525.jpg'}", "output": ""}
{"index": "526", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between these two people in image? (Unit: years)\nChoices:\n(A) 4\n(B) 5\n(C) 6\n(D) 7", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/526.jpg'}", "output": ""}
{"index": "527", "question": "What is the relative position of the two people playing guitars?\nOptions: A: They are standing next to each other against a wall, B: They are sitting on opposite sides of a table, C: They are sitting facing each other next to a window, D: They are standing far away from each other on a stage", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/527.jpg'}", "output": "c"}
{"index": "528", "question": "The object shown in this figure:\nOptions: A: Is a colorless gas with a slightly sweet odor, B: Is also known as laughing gas, C: Has a boiling point of -88.5C, D: None of these options are correct.", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/528.jpg'}", "output": "b"}
{"index": "529", "question": "The object shown in this figure:\nOptions: A: Is a highly corrosive liquid, B: Has a boiling point of 337C, C: Is used to make many types of fertilizers, D: None of these options are correct.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/529.jpg'}", "output": "d"}
{"index": "530", "question": "The object shown in this figure:\nOptions: A: Is a colorless and odorless gas, B: Has a boiling point of -161C, C: Is a greenhouse gas that contributes to climate change, D: None of these options are correct.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/530.jpg'}", "output": "d"}
{"index": "531", "question": "Which object is located directly above the man's head?\nOptions: A: Guitar, B: Tree, C: Microphone, D: Building", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/531.jpg'}", "output": "c"}
{"index": "532", "question": "The other object that is the same color as the large shiny thing is what shape?\nOptions: A: cube, B: sphere, C: cylinder, D: nan", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/532.jpg'}", "output": "c"}
{"index": "533", "question": "Where is the little girl standing in relation to the old woman in the dress in the image?\nOptions: A: Behind her, B: To the right of her, C: In front of her, D: To the left of her", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/533.jpg'}", "output": "c"}
{"index": "534", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the age gap between these two people in image? (Unit: years)\nChoices:\n(A) 5\n(B) 16\n(C) 10\n(D) 30", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/534.jpg'}", "output": "b"}
{"index": "535", "question": "Think about the magnetic force between the magnets in each pair. Which of the following statements is true?\nOptions: A: The magnitude of the magnetic force is smaller in Pair 2., B: The magnitude of the magnetic force is smaller in Pair 1., C: The magnitude of the magnetic force is the same in both pairs., D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/535.jpg'}", "output": "b"}
{"index": "536", "question": "What is the man in the image wearing on his head?\nOptions: A: Sunglasses, B: A hat, C: A headband, D: Nothing", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/536.jpg'}", "output": ""}
{"index": "537", "question": "What is the position of the woman in relation to the man in the image?\nOptions: A: In front of the man, B: Behind the man, C: To the side of the man, D: Cannot be determined", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/537.jpg'}", "output": ""}
{"index": "538", "question": "Where is the sheep?\nOptions: A: The sheep is behind the car, B: The sheep is in the front of the car, C: The sheep is on the right of the car, D: The sheep is on the left of the car", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/538.jpg'}", "output": ""}
{"index": "539", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/539.jpg'}", "output": "d"}
{"index": "540", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/540.jpg'}", "output": ""}
{"index": "541", "question": "How many people are in the car in the image?\nOptions: A: Three, B: Five, C: Four, D: Six", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/541.jpg'}", "output": "b"}
{"index": "542", "question": "Which option describe the object relationship in the image correctly?\nOptions: A: The cat is at the left side of the vase., B: The cat is inside the vase., C: The vase is facing away from the car., D: The cat is in front of the vase.", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/542.jpg'}", "output": "d"}
{"index": "543", "question": "Which of the following statements match the image?\nOptions: A: A red rectangle is below a blue ellipse., B: A cross is above an ellipse., C: A red shape is above an ellipse., D: A blue ellipse is below a red ellipse.", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/543.jpg'}", "output": "d"}
{"index": "544", "question": "What is the material of the cage-like object in this image?\nOptions: A: Plastic, B: Wire, C: Metal, D: Glass", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/544.jpg'}", "output": "c"}
{"index": "545", "question": "What color are the horses in the image?\nOptions: A: Black and white, B: Brown and white, C: Gray and brown, D: Dark brown and black", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/545.jpg'}", "output": "d"}
{"index": "546", "question": "Which image shows the highest sharpness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/546.jpg'}", "output": "b"}
{"index": "547", "question": "which direction is the dog looking at?\nOptions: A: up, B: down, C: left, D: right", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/547.jpg'}", "output": ""}
{"index": "548", "question": "What is the relative position between the apartment building and the parking lot?\nOptions: A: The building is surrounded by the parking lot., B: The parking lot is situated behind the building., C: The building is on one side of the parking lot., D: The parking lot is located in front of the building.", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/548.jpg'}", "output": "d"}
{"index": "549", "question": "The object shown in this figure:\nOptions: A: Is a colorless liquid with a slightly metallic taste, B: Is a powerful oxidizer that can cause skin and eye irritation, C: Has a boiling point of 150.2C, D: None of these options are correct.", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/549.jpg'}", "output": "b"}
{"index": "550", "question": "Which solution has a higher concentration of green particles?\nOptions: A: neither; their concentrations are the same, B: Solution B, C: Solution A, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/550.jpg'}", "output": "b"}
{"index": "551", "question": "What is the location of the microphone relative to the person playing the guitar in the image?\nOptions: A: Above and to the left, B: Below and to the right, C: Behind, D: Directly in front", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/551.jpg'}", "output": "d"}
{"index": "552", "question": "What is the position of the bulldozer with respect to the wheat field?\nOptions: A: Inside the wheat field, B: On the border of the wheat field, C: Outside the wheat field, D: Can't be determined", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/552.jpg'}", "output": "c"}
{"index": "553", "question": "What can Hazel and Xavier trade to each get what they want?\nOptions: A: Hazel can trade her tomatoes for Xavier's broccoli., B: Hazel can trade her tomatoes for Xavier's carrots., C: Xavier can trade his broccoli for Hazel's oranges., D: Xavier can trade his almonds for Hazel's tomatoes.", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/553.jpg'}", "output": ""}
{"index": "554", "question": "The object shown in this figure:\nOptions: A: Is a colorless, flammable liquid that is commonly used as a solvent and fuel, B: Has a boiling point of 64.7C, C: Can be toxic if ingested or absorbed through the skin, D: None of these options are correct.", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/554.jpg'}", "output": "c"}
{"index": "555", "question": "The gas shown in this figure:\nOptions: A: Is a colorless, odorless gas that is poisonous to humans and animals, B: Forms when fuels like gasoline, coal, and wood are burned without enough oxygen, C: Has a boiling point of -191.5C, D: None of these options are correct.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/555.jpg'}", "output": "b"}
{"index": "556", "question": "What can Abdul and Elise trade to each get what they want?\nOptions: A: Abdul can trade his tomatoes for Elise's carrots., B: Elise can trade her broccoli for Abdul's oranges., C: Elise can trade her almonds for Abdul's tomatoes., D: Abdul can trade his tomatoes for Elise's broccoli.", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/556.jpg'}", "output": "d"}
{"index": "557", "question": "What's the function of the demonstrated object?\nOptions: A: Write, B: compute, C: binding, D: copy", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/557.jpg'}", "output": "d"}
{"index": "558", "question": "What's the function of the demonstrated object?\nOptions: A: hit, B: Tighten tightly, C: adjust, D: Clamping", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/558.jpg'}", "output": "b"}
{"index": "559", "question": "What is the relative position of the window to the shutters on the building in the image?\nOptions: A: The window is above the shutters, B: The window is on the same level as the shutters, C: The window is below the shutters, D: The window is to the left of the shutters", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/559.jpg'}", "output": "c"}
{"index": "560", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/560.jpg'}", "output": ""}
{"index": "561", "question": "What's the function of the demonstrated object?\nOptions: A: baking, B: heating, C: flavouring, D: Pick-up", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/561.jpg'}", "output": ""}
{"index": "562", "question": "What type of television is present in the room?\nOptions: A: Modern, wall-mounted, B: Old, tube TV, C: Modern, smart TV, D: Old, flat screen", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/562.jpg'}", "output": ""}
{"index": "563", "question": "What can Rosa and Kylie trade to each get what they want?\nOptions: A: Kylie can trade her almonds for Rosa's tomatoes., B: Rosa can trade her tomatoes for Kylie's broccoli., C: Rosa can trade her tomatoes for Kylie's carrots., D: Kylie can trade her broccoli for Rosa's oranges.", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/563.jpg'}", "output": "b"}
{"index": "564", "question": "What is the material of the roof of the building in the image?\nOptions: A: Metal, B: Tile, C: Gray, D: Slate", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/564.jpg'}", "output": "d"}
{"index": "565", "question": "What is the relative position of the man playing the instrument to the person sitting in the chair?\nOptions: A: They are next to each other, B: They are facing each other, C: The man playing the instrument is behind the person sitting in the chair, D: Cannot be determined", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/565.jpg'}", "output": "c"}
{"index": "566", "question": "Which solution has a higher concentration of purple particles?\nOptions: A: Solution B, B: neither; their concentrations are the same, C: Solution A, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/566.jpg'}", "output": ""}
{"index": "567", "question": "Which solution has a higher concentration of pink particles?\nOptions: A: neither; their concentrations are the same, B: Solution A, C: Solution B, D: nan", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/567.jpg'}", "output": "c"}
{"index": "568", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: sample A, B: neither; the samples have the same temperature, C: sample B, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/568.jpg'}", "output": "c"}
{"index": "569", "question": "How is the model's leg positioned in the image?\nOptions: A: Crossed, B: Straight, C: Raised, D: Bent", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/569.jpg'}", "output": "c"}
{"index": "570", "question": "What is the relative position of the ocean to the pier in the image?\nOptions: A: The ocean is in front of the pier, B: The ocean is behind the pier, C: The ocean is on the left side of the pier, D: It is not specified", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/570.jpg'}", "output": ""}
{"index": "571", "question": "What direction is Syria in the Mediterranean Sea?\nOptions: A: east, B: south, C: west, D: north", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/571.jpg'}", "output": "c"}
{"index": "572", "question": "What direction is Chile in Argentina?\nOptions: A: east, B: south, C: west, D: north", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/572.jpg'}", "output": "d"}
{"index": "573", "question": "What is the position of the sun in the image?\nOptions: A: Hidden by storm clouds, B: Setting over a rocky area with rocks, C: Setting over a mountain range, D: Setting over a rocky mountain top", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/573.jpg'}", "output": "d"}
{"index": "574", "question": "What is most prominent in the image?\nOptions: A: A building with a clock, B: A city skyline, C: A boat docked in the water., D: Water with boats and ships.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/574.jpg'}", "output": ""}
{"index": "575", "question": "What is the relative position between the train and the people on the platform?\nOptions: A: The train is behind the people on the platform, B: The train is beside the people on the platform, C: The train is in front of the people on the platform, D: The train is on top of the people on the platform", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/575.jpg'}", "output": "c"}
{"index": "576", "question": "What is the relative position of the brown horse to the man in the image?\nOptions: A: The horse is behind the man, B: The horse is in front of the man, C: The horse is under the man, D: The horse is beside the man", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/576.jpg'}", "output": "b"}
{"index": "577", "question": "In this comparison picture, are the left and right modules the same color?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/577.jpg'}", "output": ""}
{"index": "578", "question": "In this comparison picture, are the left and right modules the same shape?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/578.jpg'}", "output": ""}
{"index": "579", "question": "Where is the microphone in relation to the guitar?\nOptions: A: to the right of it, B: below it, C: to the left of it, D: above it", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/579.jpg'}", "output": "c"}
{"index": "580", "question": "What is the relative position of the man holding a guitar and the woman on stage?\nOptions: A: The man is behind the woman, B: The man is to the right of the woman, C: The man is to the left of the woman, D: The man is in front of the woman", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/580.jpg'}", "output": "c"}
{"index": "581", "question": "Which option describe the object relationship in the image correctly?\nOptions: A: The handbag is on top of the bed., B: The man is attached to the bed., C: The man is lying on the bed, D: The pillows are on the bed.", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/581.jpg'}", "output": "b"}
{"index": "582", "question": "What is the object that the man with the uniform is standing in front of?\nOptions: A: A flag, B: A table, C: A picture, D: A chair", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/582.jpg'}", "output": "d"}
{"index": "583", "question": "What represents the neutral in the diagram?\nOptions: A: A, B: B, C: C, D: none of the above", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/583.jpg'}", "output": "d"}
{"index": "584", "question": "If the liquid in the picture contains only one solute, what is it most likely to contain?\nOptions: A: Ferric hydroxide., B: Sodium hydroxide., C: Sodium chloride., D: Copper sulfate.", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/584.jpg'}", "output": "c"}
{"index": "585", "question": "Which is a full grown plant?\nOptions: A: d, B: b, C: c, D: a", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/585.jpg'}", "output": ""}
{"index": "586", "question": "Which of these beaker contains water in solid condition?\nOptions: A: C, B: B, C: A, D: none", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/586.jpg'}", "output": "d"}
{"index": "587", "question": "Which picture depicts cell division?\nOptions: A: B, B: A, C: C, D: {}", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/587.jpg'}", "output": "c"}
{"index": "588", "question": "Which is right?\nOptions: A: The orange is next to the apple, B: The apple is on the left, C: The orange is on the right, D: All above are not right", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/588.jpg'}", "output": ""}
{"index": "589", "question": "Which property do these four objects have in common?\nOptions: A: sticky, B: hard, C: stretchy, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/589.jpg'}", "output": "c"}
{"index": "590", "question": "Are the two animals in the picture the same color?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/590.jpg'}", "output": ""}
{"index": "591", "question": "which label define downy woodpecker\nOptions: A: c, B: none, C: b, D: a", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/591.jpg'}", "output": "d"}
{"index": "592", "question": "Which is the final stage of a plant growth?\nOptions: A: D, B: E, C: C, D: A", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/592.jpg'}", "output": ""}
{"index": "593", "question": "What can be the relationship between the two persons in this image?\nOptions: A: Father and daughter, B: Mother and son, C: Brother and sister, D: Husband and wife", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/593.jpg'}", "output": "c"}
{"index": "594", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Is it split in half?\nChoices:\n(A) Yes\n(B) No\n(C) nan\n(D) nan", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/594.jpg'}", "output": ""}
{"index": "595", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Is the number of buss that are in front of the big yellow aeroplane less than the number of matte bicycles that are on the right side of the tiny thing?\nChoices:\n(A) Yes\n(B) No\n(C) nan\n(D) nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/595.jpg'}", "output": "b"}
{"index": "596", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: In the diagram of the food web shown what will most directly be affected by the loss of the trees?\nChoices:\n(A) horses\n(B) cats\n(C) nothing\n(D) bears", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/596.jpg'}", "output": ""}
{"index": "597", "question": "What is the relationship between the people in the image?\nOptions: A: commercial, B: professional, C: friends, D: family", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/597.jpg'}", "output": "c"}
{"index": "598", "question": "Where is the lamp in the living room?\nOptions: A: On the coffee table, B: On the windowsill, C: On the end table, D: On the floor", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/598.jpg'}", "output": ""}
{"index": "599", "question": "Which two objects are closest to each other?\nOptions: A: Soccer player and football team, B: Soccer player and coach, C: Football player and politician, D: Football player and coach", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/599.jpg'}", "output": "b"}
{"index": "600", "question": "Which is right?\nOptions: A: Two toys are next to each other, B: Two toys are far from each other, C: Two toys are facing each other, D: Two toys are backing each other", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/600.jpg'}", "output": ""}
{"index": "601", "question": "How is the sun appearing in the sky?\nOptions: A: High and bright, B: Just rising, C: Behind the couple, D: Just going down", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/601.jpg'}", "output": ""}
{"index": "602", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/602.jpg'}", "output": "b"}
{"index": "603", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/603.jpg'}", "output": ""}
{"index": "604", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/604.jpg'}", "output": "b"}
{"index": "605", "question": "Which of the following statements match the image?\nOptions: A: A triangle is to the left of a pentagon., B: A blue pentagon is to the right of a gray pentagon., C: A blue square is to the left of a blue pentagon., D: A blue pentagon is to the left of a gray shape.", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/605.jpg'}", "output": "b"}
{"index": "606", "question": "Which of the following statements match the image?\nOptions: A: A triangle is to the left of a pentagon., B: A blue pentagon is to the right of a gray pentagon., C: A blue square is to the left of a blue pentagon., D: A blue pentagon is to the left of a gray shape.", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/606.jpg'}", "output": "b"}
{"index": "607", "question": "What is the position of the sink relative to the refrigerator?\nOptions: A: The sink is on the left of the refrigerator, B: The sink is behind the refrigerator, C: The sink is in front of the refrigerator, D: The sink is on the right of the refrigerator", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/607.jpg'}", "output": "c"}
{"index": "608", "question": "Which of the following statements match the image?\nOptions: A: A rectangle is below a green ellipse., B: A blue semicircle is above a green shape., C: A green ellipse is below a yellow rectangle., D: A green ellipse is above a yellow rectangle.", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/608.jpg'}", "output": "d"}
{"index": "609", "question": "Which of the following statements match the image?\nOptions: A: A triangle is to the right of a blue rectangle., B: A magenta triangle is to the left of a blue rectangle., C: A magenta rectangle is to the left of a magenta shape., D: A yellow triangle is to the right of a blue shape.", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/609.jpg'}", "output": "b"}
{"index": "610", "question": "The object shown in this figure:\nOptions: A: Is a colorless liquid with a sweet, fruity odor, B: Has a boiling point of 56.05C, C: Is used as a solvent for many organic compounds, D: None of these options are correct.", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/610.jpg'}", "output": "d"}
{"index": "611", "question": "Which object is closest to the crowd in the image?\nOptions: A: Airliner, B: Train, C: Jet, D: None of the above", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/611.jpg'}", "output": ""}
{"index": "612", "question": "What is the position of the picture on the wall in relation to the window?\nOptions: A: Above the window, B: To the right of the window, C: To the left of the window, D: Below the window", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/612.jpg'}", "output": ""}
{"index": "613", "question": "Which of the following statements match the image?\nOptions: A: A triangle is to the right of an ellipse., B: A triangle is to the left of an ellipse., C: A green cross is to the right of a red shape., D: A green triangle is to the left of a yellow ellipse.", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/613.jpg'}", "output": "d"}
{"index": "614", "question": "Where is the rug in the living room located?\nOptions: A: Next to the door, B: Under the table, C: In front of the window, D: Under the couch", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/614.jpg'}", "output": "d"}
{"index": "615", "question": "Which object is bigger, the Christmas tree or the hotel lobby?\nOptions: A: They are about the same size, B: Hotel lobby, C: Christmas tree, D: Cannot be determined from the given information", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/615.jpg'}", "output": "c"}
{"index": "616", "question": "What is the relative position of the spatula to the stack of plates?\nOptions: A: Above the stack of plates, B: Below the stack of plates, C: To the left of the stack of plates, D: To the right of the stack of plates", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/616.jpg'}", "output": "c"}
{"index": "617", "question": "What is the most curved beak species?\nOptions: A: iiki, B: swallow-tanager, C: cliff swallow, D: hawfinch", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/617.jpg'}", "output": "b"}
{"index": "618", "question": "Where is the woman's blue bag located in the image?\nOptions: A: In her hand, B: On her shoulder, C: On the ground, D: Inside the man's bag", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/618.jpg'}", "output": "b"}
{"index": "619", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: neither; the samples have the same temperature, B: sample B, C: sample A, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/619.jpg'}", "output": "b"}
{"index": "620", "question": "Which is lobed leaf?\nOptions: A: E, B: A, C: C, D: B", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/620.jpg'}", "output": ""}
{"index": "621", "question": "Which group has a notched outline?\nOptions: A: A, B: D, C: B, D: C", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/621.jpg'}", "output": "c"}
{"index": "622", "question": "What is the position of the man in regards to the drum set?\nOptions: A: In front of the drum set, B: To the right of the drum set, C: Behind the drum set, D: To the left of the drum set", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/622.jpg'}", "output": "c"}
{"index": "623", "question": "Which property do these four objects have in common?\nOptions: A: fragile, B: transparent, C: opaque, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/623.jpg'}", "output": ""}
{"index": "624", "question": "In this picture, are the two dolphins the same size?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/624.jpg'}", "output": "b"}
{"index": "625", "question": "Which of the insects is an ant?\nOptions: A: Insect A, B: Insect C, C: All, D: Insect D", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/625.jpg'}", "output": "b"}
{"index": "626", "question": "Based on the image, where is the boy?\nOptions: A: The boy is on the right of the fire hydrant, B: The boy is on the left of the fire hydrant, C: The boy is on the top of the fire hydrant, D: All above are not right", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/626.jpg'}", "output": "b"}
{"index": "627", "question": "Which solution has a higher concentration of yellow particles?\nOptions: A: Solution A, B: neither; their concentrations are the same, C: Solution B, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/627.jpg'}", "output": "c"}
{"index": "628", "question": "which stage denotes larvae ?\nOptions: A: A, B: B, C: C, D: D", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/628.jpg'}", "output": ""}
{"index": "629", "question": "Which solution has a higher concentration of blue particles?\nOptions: A: Solution A, B: Solution B, C: neither; their concentrations are the same, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/629.jpg'}", "output": "b"}
{"index": "630", "question": "Which of the following statements match the image?\nOptions: A: A gray circle is to the left of a cyan shape., B: A cyan square is to the left of a gray circle., C: A cyan ellipse is to the right of a gray circle., D: A cyan circle is to the right of a circle.", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/630.jpg'}", "output": "b"}
{"index": "631", "question": "What is the relationship between the people in the image?\nOptions: A: commercial, B: professional, C: friends, D: family", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/631.jpg'}", "output": "c"}
{"index": "632", "question": "Which solution has a higher concentration of purple particles?\nOptions: A: Solution A, B: Solution B, C: neither; their concentrations are the same, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/632.jpg'}", "output": ""}
{"index": "633", "question": "Which attribute best describes the hair of a man in the image?\nOptions: A: Long and curly, B: Black and wavy, C: Blonde and short, D: Bald", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/633.jpg'}", "output": "c"}
{"index": "634", "question": "Which solution has a higher concentration of blue particles?\nOptions: A: Solution B, B: neither; their concentrations are the same, C: Solution A, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/634.jpg'}", "output": "c"}
{"index": "635", "question": "Which solution has a higher concentration of purple particles?\nOptions: A: neither; their concentrations are the same, B: Solution A, C: Solution B, D: nan", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/635.jpg'}", "output": "c"}
{"index": "636", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: How many people prefer the most preferred object?\nChoices:\n(A) 14\n(B) 5\n(C) 7\n(D) 9", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/636.jpg'}", "output": "c"}
{"index": "637", "question": "Where is the top of the purple bottle in relation to the man with the guitar?\nOptions: A: Beside him, B: Above him, C: Below him, D: On his head", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/637.jpg'}", "output": ""}
{"index": "638", "question": "Which action is performed in this image?\nOptions: A: swimming backstroke, B: jumping into pool, C: situp, D: water sliding", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/638.jpg'}", "output": "c"}
{"index": "639", "question": "Which of these faults is the most common/normal?\nOptions: A: C, B: A, C: B, D: None of the above.", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/639.jpg'}", "output": "b"}
{"index": "640", "question": "Where are the drawers located in the image?\nOptions: A: Under the bench, B: Above the sink, C: Next to the cabinets, D: Under the countertop", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/640.jpg'}", "output": ""}
{"index": "641", "question": "In which organ digestion is completed?\nOptions: A: B, B: D, C: E, D: None of above", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/641.jpg'}", "output": ""}
{"index": "642", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: sample A, B: sample B, C: neither; the samples have the same temperature, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/642.jpg'}", "output": "b"}
{"index": "643", "question": "Where is the man playing the guitar located on the stage?\nOptions: A: Left side of the stage, B: Middle of the stage, C: Right side of the stage, D: Behind the microphones", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/643.jpg'}", "output": "b"}
{"index": "644", "question": "What is the relation between a speaker and the wall in the image?\nOptions: A: The speaker is on the wall, B: The speaker is standing in front of the wall, C: The speaker is attached to the wall, D: The speaker is behind the wall", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/644.jpg'}", "output": "d"}
{"index": "645", "question": "Which property do these three objects have in common?\nOptions: A: bumpy, B: bouncy, C: shiny, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/645.jpg'}", "output": ""}
{"index": "646", "question": "Who feeds on grass?\nOptions: A: Deer, B: Moose, C: Cottontail, D: None of the above", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/646.jpg'}", "output": "c"}
{"index": "647", "question": "Which solution has a higher concentration of blue particles?\nOptions: A: Solution A, B: Solution B, C: neither; their concentrations are the same, D: nan", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/647.jpg'}", "output": "c"}
{"index": "648", "question": "What is the main instrument being played on stage?\nOptions: A: Saxophone, B: Drums, C: Guitar, D: Violin", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/648.jpg'}", "output": "c"}
{"index": "649", "question": "Is there a rug on the floor of the dining room?\nOptions: A: Yes, B: Not mentioned in the information provided, C: Cannot be determined, D: No", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/649.jpg'}", "output": "d"}
{"index": "650", "question": "What is the position of the window in the sitting room?\nOptions: A: above the couch, B: beside the couch, C: below the couch, D: opposite of the couch", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/650.jpg'}", "output": "d"}
{"index": "651", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: sample A, B: sample B, C: neither; the samples have the same temperature, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/651.jpg'}", "output": "b"}
{"index": "652", "question": "What is the position of the white couch relative to the glass coffee table?\nOptions: A: The couch is in front of the coffee table, B: The couch is to the right of the coffee table, C: The couch is to the left of the coffee table, D: The couch is behind the coffee table", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/652.jpg'}", "output": ""}
{"index": "653", "question": "In what relative position are the men playing the guitars on stage?\nOptions: A: Standing close to each other, B: Standing apart from each other, C: Sitting apart from each other, D: Sitting close to each other", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/653.jpg'}", "output": "b"}
{"index": "654", "question": "What the nature relations of these animals\nOptions: A: predation, B: mutualism, C: parasitism, D: nan", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/654.jpg'}", "output": "b"}
{"index": "655", "question": "How many kids of feet are there?\nOptions: A: 4, B: 2, C: 6, D: 5", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/655.jpg'}", "output": "c"}
{"index": "656", "question": "What is the predominant color of the sky in the image?\nOptions: A: Blue, B: Gray, C: Yellow, D: Orange", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/656.jpg'}", "output": ""}
{"index": "657", "question": "What is the position of the wine bottle(s) relative to the wine glass in the image?\nOptions: A: The wine bottle(s) is on the left of the wine glass, B: The wine bottle(s) is behind the wine glass, C: The wine bottle(s) is on the right of the wine glass, D: It is not clear from the image", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/657.jpg'}", "output": ""}
{"index": "658", "question": "Which plant is the biggest\nOptions: A: first, B: No difference, C: Second, D: Third", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/658.jpg'}", "output": "b"}
{"index": "659", "question": "In the picture there are two objects stacked with cubes. Are they the same shape?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/659.jpg'}", "output": ""}
{"index": "660", "question": "There are two physical models in the picture, are the two square sliders the same size?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/660.jpg'}", "output": "b"}
{"index": "661", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/661.jpg'}", "output": "b"}
{"index": "662", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Among the states that border Georgia , does Florida have the lowest value ?\nChoices:\n(A) Yes\n(B) No\n(C) nan\n(D) nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/662.jpg'}", "output": "b"}
{"index": "663", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: neither; the samples have the same temperature, B: sample A, C: sample B, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/663.jpg'}", "output": "c"}
{"index": "664", "question": "Which option describe the object relationship in the image correctly?\nOptions: A: The sink contains the cat., B: The cat is beside the microwave., C: The cat is at the edge of the sink., D: The book is beside the cat.", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/664.jpg'}", "output": ""}
{"index": "665", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/665.jpg'}", "output": ""}
{"index": "666", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: sample A, B: sample B, C: neither; the samples have the same temperature, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/666.jpg'}", "output": "b"}
{"index": "667", "question": "What is the position of the glass of juice relative to the tangerines?\nOptions: A: Next to the tangerines, B: Between the tangerines, C: Behind the tangerines, D: Cannot tell", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/667.jpg'}", "output": ""}
{"index": "668", "question": "Which image shows the highest sharpness?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/668.jpg'}", "output": ""}
{"index": "669", "question": "Which image is the brightest one?\nOptions: A: upper left, B: upper right, C: down left, D: down right", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/669.jpg'}", "output": "d"}
{"index": "670", "question": "Where is the police officer located in the image?\nOptions: A: In the crowd, B: In front of a crowd, C: Far away from the crowd, D: Not present in the image", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/670.jpg'}", "output": ""}
{"index": "671", "question": "Which are is the largest and most anterior part of each cerebral hemisphere?\nOptions: A: E, B: A, C: D, D: I", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/671.jpg'}", "output": "d"}
{"index": "672", "question": "What the nature relations of these animals\nOptions: A: predation, B: mutualism, C: parasitism, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/672.jpg'}", "output": "b"}
{"index": "673", "question": "Which solution has a higher concentration of blue particles?\nOptions: A: Solution B, B: neither; their concentrations are the same, C: Solution A, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/673.jpg'}", "output": "c"}
{"index": "674", "question": "Which object is located closer to the road - the gas station or the parking lot?\nOptions: A: They are the same distance from the road, B: Parking lot, C: Gas station, D: None of the above", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/674.jpg'}", "output": "b"}
{"index": "675", "question": "What is the main activity of the two babies in the image?\nOptions: A: Eating toys on the floor, B: Playing with each other, C: Crying and crawling, D: Sitting still and staring", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/675.jpg'}", "output": ""}
{"index": "676", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: sample B, B: sample A, C: neither; the samples have the same temperature, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/676.jpg'}", "output": ""}
{"index": "677", "question": "In this comparison diagram, are the upper and lower modules the same shape?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/677.jpg'}", "output": ""}
{"index": "678", "question": "Compare the average kinetic energies of the particles in each sample. Which sample has the higher temperature?\nOptions: A: sample B, B: neither; the samples have the same temperature, C: sample A, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'ScienceQA_TEST', 'split': 'val', 'image_path': 'images/678.jpg'}", "output": ""}
{"index": "679", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/679.jpg'}", "output": ""}
{"index": "680", "question": "What is the relative position of the man and the woman sitting at the table?\nOptions: A: The man is on the left and the woman is on the right, B: The man is on the right and the woman is on the left, C: The man and the woman are facing each other, D: The man and the woman are back-to-back", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/680.jpg'}", "output": "c"}
{"index": "681", "question": "What is the relative position of the man and woman in the image?\nOptions: A: They are facing each other, B: They are both facing forward, C: They are both facing the camera, D: They are both looking away from the camera", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/681.jpg'}", "output": "b"}
{"index": "682", "question": "What is the size of the leaf in the top right corner of the image relative to the white paper below it?\nOptions: A: Cannot be determined, B: Smaller, C: Same size, D: Larger", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/682.jpg'}", "output": "d"}
{"index": "683", "question": "Which of the following statements match the image?\nOptions: A: A triangle is to the right of an ellipse., B: A triangle is to the left of a red ellipse., C: A cyan shape is to the right of a red ellipse., D: A red square is to the left of a green triangle.", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/683.jpg'}", "output": "b"}
{"index": "684", "question": "What's the profession of the people in this picture?\nOptions: A: mason, B: plumber, C: pilot, D: police", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/684.jpg'}", "output": "b"}
{"index": "685", "question": "What's the function of the demonstrated object?\nOptions: A: Separatist, B: Clamping, C: drill, D: incise", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/685.jpg'}", "output": "d"}
{"index": "686", "question": "What's the function of the demonstrated object?\nOptions: A: gluing, B: Receive, C: Stationery, D: record", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/686.jpg'}", "output": "c"}
{"index": "687", "question": "The object shown in this figure:\nOptions: A: Is a colorless gas with a pungent odor, B: Is commonly used as a fertilizer and industrial chemical, C: Has a boiling point of -33.3C, D: None of these options are correct.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/687.jpg'}", "output": ""}
{"index": "688", "question": "In what situations would the scene in the picture appear?\nOptions: A: Put a piece of iron into water., B: Put a piece of plastic into water., C: Put a piece of sodium into water., D: Put a piece of sodium into kerosene.", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/688.jpg'}", "output": "c"}
{"index": "689", "question": "What's the function of the demonstrated object?\nOptions: A: grill, B: filtration, C: flavouring, D: Pick-up", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/689.jpg'}", "output": "d"}
{"index": "690", "question": "What's the function of the demonstrated object?\nOptions: A: Cut vegetables, B: stir, C: Water purification, D: Boiling water", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/690.jpg'}", "output": "d"}
{"index": "691", "question": "Which property do these two objects have in common?\nOptions: A: yellow, B: salty, C: nan, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/691.jpg'}", "output": "b"}
{"index": "692", "question": "What's the function of the demonstrated object?\nOptions: A: hit, B: Tighten tightly, C: adjust, D: Clamping", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/692.jpg'}", "output": ""}
{"index": "693", "question": "what is the shape of this object?\nOptions: A: circle, B: triangle, C: square, D: rectangle", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/693.jpg'}", "output": "c"}
{"index": "694", "question": "Which is the main persuasive appeal used in this ad?\nOptions: A: logos (reason), B: pathos (emotion), C: ethos (character), D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/694.jpg'}", "output": "d"}
{"index": "695", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/695.jpg'}", "output": "d"}
{"index": "696", "question": "What's the function of the demonstrated object?\nOptions: A: excavate, B: transport, C: weld, D: Measure the level", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/696.jpg'}", "output": ""}
{"index": "697", "question": "What's the function of the demonstrated object?\nOptions: A: excavate, B: transport, C: weld, D: Measure the level", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/697.jpg'}", "output": "b"}
{"index": "698", "question": "What's the function of the demonstrated object?\nOptions: A: clean, B: measurement, C: Bulldozing, D: Cutting platform", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/698.jpg'}", "output": "d"}
{"index": "699", "question": "What's the function of the demonstrated object?\nOptions: A: increase passenger capacity and reduce traffic congestion, B: a sanitary facility used for excretion, C: used as decorations., D: watch TV shows", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/699.jpg'}", "output": ""}
{"index": "700", "question": "What's the function of the demonstrated object?\nOptions: A: prepare food and cook meals, B: sleep, C: a sanitary facility used for excretion, D: Play basketball", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/700.jpg'}", "output": "b"}
{"index": "701", "question": "What's the profession of the people in this picture?\nOptions: A: driver, B: teacher, C: waiter, D: tailor", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/701.jpg'}", "output": "c"}
{"index": "702", "question": "What job is the person in the image most likely to do?\nOptions: A: police officer, B: nurse, C: fireman, D: farmer", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/702.jpg'}", "output": ""}
{"index": "703", "question": "What's the function of the demonstrated object?\nOptions: A: display information in pictorial or textual form, B: project images or videos onto a larger surface, C: watch TV shows, D: display digital photos in a slideshow format.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/703.jpg'}", "output": ""}
{"index": "704", "question": "What's the function of the demonstrated object?\nOptions: A: entertainment and scientific research, B: bind papers together, C: hitting things, D: tighten or loosen screws", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/704.jpg'}", "output": "c"}
{"index": "705", "question": "What's the profession of the people in this picture?\nOptions: A: baker, B: butcher, C: carpenter, D: designer", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/705.jpg'}", "output": "c"}
{"index": "706", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/706.jpg'}", "output": "d"}
{"index": "707", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/707.jpg'}", "output": "d"}
{"index": "708", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/708.jpg'}", "output": "b"}
{"index": "709", "question": "What's the function of the demonstrated object?\nOptions: A: Cut the grass, B: Measure the temperature, C: burnish, D: Brushing", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/709.jpg'}", "output": "b"}
{"index": "710", "question": "What's the function of the demonstrated object?\nOptions: A: excavate, B: transport, C: weld, D: Measure the level", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/710.jpg'}", "output": "c"}
{"index": "711", "question": "What's the function of the demonstrated object?\nOptions: A: grill, B: filtration, C: flavouring, D: Pick-up", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/711.jpg'}", "output": "c"}
{"index": "712", "question": "What's the function of the demonstrated object?\nOptions: A: grill, B: filtration, C: flavouring, D: Pick-up", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/712.jpg'}", "output": "b"}
{"index": "713", "question": "What's the profession of the people in this picture?\nOptions: A: magician, B: financial analyst, C: florist, D: lawyer", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/713.jpg'}", "output": ""}
{"index": "714", "question": "What's the profession of the people in this picture?\nOptions: A: photographer, B: dancer, C: writer, D: architect", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/714.jpg'}", "output": "d"}
{"index": "715", "question": "What's the profession of the people in this picture?\nOptions: A: photographer, B: chemist, C: repairman, D: pianist", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/715.jpg'}", "output": "c"}
{"index": "716", "question": "What's the profession of the people in this picture?\nOptions: A: mason, B: nurse, C: pilot, D: policeman", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/716.jpg'}", "output": "d"}
{"index": "717", "question": "What's the profession of the people in this picture?\nOptions: A: mason, B: postman, C: pilot, D: policeman", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/717.jpg'}", "output": "b"}
{"index": "718", "question": "What's the profession of the people in this picture?\nOptions: A: mason, B: postman, C: singer, D: soldier", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/718.jpg'}", "output": "d"}
{"index": "719", "question": "What's the profession of the people in this picture?\nOptions: A: astronaut, B: chemist, C: musician, D: pianist", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/719.jpg'}", "output": ""}
{"index": "720", "question": "What's the profession of the people in this picture?\nOptions: A: astronaut, B: chemist, C: violinist, D: pianist", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/720.jpg'}", "output": "c"}
{"index": "721", "question": "What's the profession of the people on the right?\nOptions: A: fashion designer, B: accountant, C: dentist, D: architect", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/721.jpg'}", "output": "c"}
{"index": "722", "question": "What's the function of the demonstrated object?\nOptions: A: Cut vegetables, B: stir, C: Water purification, D: Boiling water", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/722.jpg'}", "output": ""}
{"index": "723", "question": "What's the profession of the people in this picture?\nOptions: A: detective, B: accountant, C: cashier, D: architect", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/723.jpg'}", "output": "c"}
{"index": "724", "question": "What's the profession of the people in this picture?\nOptions: A: farmer, B: fireman, C: hairdresser, D: judge", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/724.jpg'}", "output": "d"}
{"index": "725", "question": "What's the profession of the people in this picture?\nOptions: A: mason, B: nurse, C: hairdresser, D: judge", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/725.jpg'}", "output": "b"}
{"index": "726", "question": "What's the profession of the people in this picture?\nOptions: A: driver, B: teacher, C: electrician, D: tailor", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/726.jpg'}", "output": "c"}
{"index": "727", "question": "What's the profession of the people in this picture?\nOptions: A: driver, B: chemist, C: janitor, D: tailor", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/727.jpg'}", "output": "b"}
{"index": "728", "question": "What's the profession of the people in this picture?\nOptions: A: baker, B: butcher, C: carpenter, D: doctor", "answer": "D", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/728.jpg'}", "output": "d"}
{"index": "729", "question": "Which special day is associated with this poster?\nOptions: A: Earth Day., B: National Reading Day., C: Father's Day., D: Mother's Day", "answer": "C", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/729.jpg'}", "output": "c"}
{"index": "730", "question": "Which special day is associated with this poster?\nOptions: A: Earth Day., B: Children's Day., C: Father's Day., D: Mother's Day", "answer": "B", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/730.jpg'}", "output": "b"}
{"index": "731", "question": "What is the relationship between the people in the image?\nOptions: A: friends, B: family, C: commercial, D: professional", "answer": "D", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/731.jpg'}", "output": "d"}
{"index": "732", "question": "What does this sign mean?\nOptions: A: Smoking is prohibited here., B: Something is on sale., C: No photography allowed, D: Take care of your speed.", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/732.jpg'}", "output": "c"}
{"index": "733", "question": "which image is more colorful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/733.jpg'}", "output": "b"}
{"index": "734", "question": "Which option describe the object relationship in the image correctly?\nOptions: A: The bed is beneath the suitcase., B: The car is behind the suitcase., C: The suitcase is beneath the bed., D: The cat is on the microwave.", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/734.jpg'}", "output": ""}
{"index": "735", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/735.jpg'}", "output": "b"}
{"index": "736", "question": "Which rhetorical appeal is primarily used in this ad?\nOptions: A: ethos (character), B: pathos (emotion), C: logos (reason), D: nan", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/736.jpg'}", "output": "c"}
{"index": "737", "question": "Based on the image, what is the significance of having two cakes on the table during the couple's celebration with their baby?\nOptions: A: Having two cakes allows for different cake flavors or designs for their guests., B: Having two cakes signifies that the couple is celebrating multiple occasions or milestones., C: Having two cakes indicates a preference for abundance and excess., D: Having two cakes is a common practice in most celebrations of this nature.", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/737.jpg'}", "output": "d"}
{"index": "738", "question": "The object shown in this figure:\nOptions: A: Is a colorless, odorless gas., B: Can be ionized to produce a plasma., C: Has a high boiling point compared to other noble gases., D: Is the most abundant element in the universe.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/738.jpg'}", "output": "d"}
{"index": "739", "question": "The object shown in this figure:\nOptions: A: Is a colorless liquid with a sharp odor, B: Can be used as a fertilizer for plants, C: Has a pH value of less than 7, D: None of these options are correct.", "answer": "A", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/739.jpg'}", "output": "d"}
{"index": "740", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/740.jpg'}", "output": "b"}
{"index": "741", "question": "Which property do these three objects have in common?\nOptions: A: blue, B: smooth, C: flexible, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance relation reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/741.jpg'}", "output": "b"}
{"index": "742", "question": "What's the function of the demonstrated object?\nOptions: A: Cooking, B: Cook soup, C: Fry, D: steam", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/742.jpg'}", "output": "c"}
{"index": "743", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "A", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/743.jpg'}", "output": ""}
{"index": "744", "question": "Which image is more brightful?\nOptions: A: The first image, B: The second image, C: nan, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/744.jpg'}", "output": "b"}
{"index": "745", "question": "In the picture, one is a bear doll and the other is a cat. Are they the same size?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/745.jpg'}", "output": "b"}
{"index": "746", "question": "In this comparison picture, are the upper and lower modules the same color?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/746.jpg'}", "output": "b"}
{"index": "747", "question": "What's the function of the demonstrated object?\nOptions: A: Draw, B: cut, C: deposit, D: refrigeration", "answer": "C", "category": "instance reasoning", "l2_category": "single-instance reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/747.jpg'}", "output": "c"}
{"index": "748", "question": "In the picture, one is a bear doll and the other is a cat. Are they the same size?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/748.jpg'}", "output": "b"}
{"index": "749", "question": "In this comparison picture, are the upper and lower modules the same color?\nOptions: A: same, B: Not the same, C: Can't judge, D: nan", "answer": "B", "category": "instance reasoning", "l2_category": "cross-instance attribute reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/749.jpg'}", "output": "b"}
{"index": "750", "question": "What is the age group of the people in this image generally aimed at?\nOptions: A: Middle-aged people, B: Teenagers, C: Children, D: Elderly people", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/750.jpg'}", "output": ""}
{"index": "751", "question": "<image 1> is the HRM Roles, David Ulrich (1997) model. According to Ulrich, if HR is to become and remain relevant in the current business environment, what is not something HR professionals must learn?\nOptions: A: Manage human resources operations and infrastructure, B: Become the link between human resources and organizational strategy, C: Optimize the value of employees, D: Be a change agent with a long-term vision for the organization", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MMMU', 'split': 'val', 'image_path': 'images/751.jpg'}", "output": ""}
{"index": "752", "question": "What could be the reason for some people in the image appearing blurry?\nOptions: A: Camera malfunction, B: Distance from the camera, C: They are moving too fast, D: Intentional blur", "answer": "D", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/752.jpg'}", "output": "b"}
{"index": "753", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: How many models in the table have a model size larger than 10B?\nChoices:\n(A) 1\n(B) 8\n(C) 10\n(D) 11", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/753.jpg'}", "output": ""}
{"index": "754", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What time is shown? Answer by typing a time word, not a number. It is (_) to eight.\nChoices:\n(A) half\n(B) quarter\n(C) o'clock\n(D) quarter to", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/754.jpg'}", "output": "b"}
{"index": "755", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Subtract all tiny shiny balls. Subtract all purple objects. How many objects are left?\nChoices:\n(A) 4\n(B) 8\n(C) 2\n(D) 6", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/755.jpg'}", "output": "d"}
{"index": "756", "question": "Where is the woman holding her belly in the image?\nOptions: A: With both hands, B: With her right hand, C: With her left hand, D: She's not holding her belly", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/756.jpg'}", "output": "c"}
{"index": "757", "question": "In which direction is the player jumping over the barrier?\nOptions: A: Left to right, B: Right to left, C: Towards the camera, D: Away from the camera", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/757.jpg'}", "output": ""}
{"index": "758", "question": "What is the basketball player on the left doing in the image?\nOptions: A: Dribbling the ball, B: Throwing the ball to a teammate, C: Trying to tackle the other player, D: Standing still", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/758.jpg'}", "output": "c"}
{"index": "759", "question": "How many types of fruits are there in the image?\nOptions: A: 3, B: 2, C: 5, D: 4", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/759.jpg'}", "output": ""}
{"index": "760", "question": "Which corner doesn't have any food?\nOptions: A: top-right, B: top-left, C: bottom-left, D: bottom-right", "answer": "D", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/760.jpg'}", "output": "c"}
{"index": "761", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: How many methods in the table achieve an A-847 score higher than 20.0?\nChoices:\n(A) 0\n(B) 1\n(C) 2\n(D) 3", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/761.jpg'}", "output": "b"}
{"index": "762", "question": "In the picture, which direction is the baby facing?\nOptions: A: left, B: right, C: up, D: down", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/762.jpg'}", "output": "b"}
{"index": "763", "question": "What is the position of the person on stage with the guitar relative to the night sky?\nOptions: A: In front of the night sky, B: Behind the night sky, C: At the same level as the night sky, D: There is no visible night sky", "answer": "D", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/763.jpg'}", "output": ""}
{"index": "764", "question": "What type of vegetation is visible in the image?\nOptions: A: Trees only, B: A mix of trees and bushes, C: Bushes only, D: Grass", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/764.jpg'}", "output": "b"}
{"index": "765", "question": "How many predators does golden algae have?\nOptions: A: 4, B: 5, C: 3, D: 6", "answer": "B", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/765.jpg'}", "output": "c"}
{"index": "766", "question": "Where is the young man in the black robe seated in the painting?\nOptions: A: On a bench, B: None of the above, C: In front of a sculpture, D: On the ground", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/766.jpg'}", "output": "c"}
{"index": "767", "question": "How many people are playing instruments in the image?\nOptions: A: 4, B: 3, C: 2, D: 5", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/767.jpg'}", "output": "d"}
{"index": "768", "question": "How many glasses are present in the image?\nOptions: A: 0, B: 3, C: 2, D: 1", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/768.jpg'}", "output": "c"}
{"index": "769", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Subtract all yellow metallic balls. Subtract all small yellow shiny things. How many objects are left?\nChoices:\n(A) 4\n(B) 5\n(C) 6\n(D) 8", "answer": "D", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/769.jpg'}", "output": ""}
{"index": "770", "question": "Approximately what proportion of the picture is occupied by the bus in the image?\nOptions: A: 0.8, B: 1, C: 0.6, D: 0.3", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/770.jpg'}", "output": "b"}
{"index": "771", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: How many years have value less than 10%?\nChoices:\n(A) 0\n(B) 1\n(C) 2\n(D) 5", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/771.jpg'}", "output": "b"}
{"index": "772", "question": "Based on the event chain, when is Tinker Bell poisoned?\nOptions: A: before Captain Hook captures the Lost Boys, B: after the Lost Boys fight the pirates, C: nan, D: nan", "answer": "B", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/772.jpg'}", "output": ""}
{"index": "773", "question": "According to the given food web, which is the animal that eats other animals and also becomes a prey to other animals?\nOptions: A: salmon, B: deer, C: moose, D: none of the above", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/773.jpg'}", "output": "c"}
{"index": "774", "question": "Which Python code can generate the content of the image?\nOptions: A: count = 0\nwhile (count < 10):\nprint 'The count is:', count\ncount = count + 1\n\nprint \"Good bye!\", B: count = 0\nwhile (count < 9):\nprint 'The count is:', count\ncount = count + 1\n\nprint \"Good bye!\", C: count = 1\nwhile (count < 9):\nprint 'The count is:', count\ncount = count + 1\n\nprint \"Good bye!\", D: count = 0\nwhile (count < 9):\nprint 'The count is:', count\ncount = count + 2\n\nprint \"Good bye!\"", "answer": "B", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/774.jpg'}", "output": ""}
{"index": "775", "question": "Which Python code can generate the content of the image?\nOptions: A: def get_vowels(string):\nreturn [vowel for vowel in string if vowel in 'aeiou'] \nprint(\"Vowels are:\", get_vowels('This is some random string')), B: def get_vowels(string):\nreturn [vowel for vowel in string if vowel in 'weiou'] \nprint(\"Vowels are:\", get_vowels('This is some random string')), C: def get_vowels(string):\nreturn [vowel for vowel in string if vowel in 'aeiou'] \nprint(\"Vowels are:\", get_vowels('This is a string')), D: def get_vowels(string):\nreturn [vowel for vowel in string if vowel in 'aeiou'] \nprint(\"Vowels are:\", get_vowels('string'))", "answer": "A", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/775.jpg'}", "output": ""}
{"index": "776", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the difference between the highest and the lowest dark blue bar?\n(A) 54\n(B) 52\n(C) 56\n(D) 57", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/776.jpg'}", "output": ""}
{"index": "777", "question": "H+B53int: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Choose the answer for the missing picture.\nChoices:\n(A) 1\n(B) 2\n(C) 3\n(D) 4", "answer": "D", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/777.jpg'}", "output": "d"}
{"index": "778", "question": "what python code is gonna generate the result as shown in the image?\nOptions: A: if 5 > 2:\nprint(\"Five is greater than two!\")\nprint(\"Five is greater than two!\"), B: if 5 > 2:\nprint(\"Five is greater than two!\") \nif 5 > 2:\nprint(\"Five is greater than two!\"), C: #This is a comment.\nprint(\"Hello, World!\"), D: if 5 > 2:\nprint(\"Five is greater than two!\")", "answer": "A", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/778.jpg'}", "output": "b"}
{"index": "779", "question": "What is the image primarily displaying?\nOptions: A: Architecture, B: Animals, C: Interior design, D: Landscaping", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/779.jpg'}", "output": ""}
{"index": "780", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What time is shown? Answer by typing a time word, not a number. It is (_) after nine.\nChoices:\n(A) half\n(B) quarter\n(C) o'clock\n(D) quarter to", "answer": "B", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/780.jpg'}", "output": "b"}
{"index": "781", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Which number is missing?\nChoices:\n(A) 40\n(B) 10\n(C) 22\n(D) 34", "answer": "A", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/781.jpg'}", "output": "d"}
{"index": "782", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Which number comes next?\nChoices:\n(A) 2023\n(B) 2123\n(C) 2223\n(D) 2133", "answer": "B", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/782.jpg'}", "output": ""}
{"index": "783", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Calculate the missing item.\nChoices:\n(A) 0\n(B) 1\n(C) 2\n(D) 3", "answer": "B", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/783.jpg'}", "output": "b"}
{"index": "784", "question": "what python code is gonna generate the result as shown in the image?\nOptions: A: for x in \"banana\":\nprint(x), B: fruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\nprint(x) \nif x == \"banana\":\nbreak, C: fruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\nif x == \"banana\":\nbreak\nprint(x), D: fruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\nif x == \"banana\":\ncontinue\nprint(x)", "answer": "D", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/784.jpg'}", "output": "b"}
{"index": "785", "question": "Which Python code can generate the content of the image?\nOptions: A: from collections import Counter\nresult = Counter('Canada')\nprint(result), B: from collections import Counter\nresult = Counter('strawberry')\nprint(result), C: from collections import Counter\nresult = Counter('banana')\nprint(result), D: from collections import Counter\nresult = Counter('apple')\nprint(result)", "answer": "C", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/785.jpg'}", "output": ""}
{"index": "786", "question": "Which Python code can generate the content of the image?\nOptions: A: tup1 = ('physics', 'chemistry', 1997, 2000)\ntup2 = (1, 2, 3, 4, 5, 6, 7 )\nprint \"tup1[0]: \", tup1[0]\nprint \"tup2[1:5]: \", tup2[1:5], B: tup1 = ('physics', 'chemistry', 1997, 2000)\ntup2 = (1, 2, 3, 4, 5, 6, 7 )\nprint \"tup1[0]: \", tup1[0]\nprint \"tup2[1:5]: \", tup2[2:5], C: tup1 = ('physics', 'chemistry', 1990, 2000)\ntup2 = (1, 2, 3, 4, 5, 6, 7 )\nprint \"tup1[0]: \", tup1[0]\nprint \"tup2[1:5]: \", tup2[1:5], D: tup1 = ('physics', 'chemistry', 1990, 2000)\ntup2 = (1, 2, 3, 4, 5, 6, 7 )\nprint \"tup1[0]: \", tup1[0]\nprint \"tup2[2:5]: \", tup2[1:5]", "answer": "A", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/786.jpg'}", "output": "b"}
{"index": "787", "question": "Which Python code can generate the content of the image?\nOptions: A: print str \nprint str[0] \nprint str[2:5] \nprint str[2:] \nprint str * 2 \nprint str + \"TEST\", B: print str \nprint str[1] \nprint str[2:5] \nprint str[2:] \nprint str * 2 \nprint str + \"TEST\", C: print str \nprint str[0] \nprint str[1:5] \nprint str[2:] \nprint str * 2 \nprint str + \"TEST\", D: print str \nprint str[0] \nprint str[2:5] \nprint str[3:] \nprint str * 2 \nprint str + \"TEST\"", "answer": "A", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/787.jpg'}", "output": "c"}
{"index": "788", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Find the value of the square in the figure.\nChoices:\n(A) 0\n(B) 2\n(C) 1\n(D) 3", "answer": "B", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/788.jpg'}", "output": "c"}
{"index": "789", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What time is shown? Answer by typing a time word, not a number. It is (_) past six.\nChoices:\n(A) half\n(B) quarter\n(C) o'clock\n(D) quarter to", "answer": "B", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/789.jpg'}", "output": "b"}
{"index": "790", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Which of the following domains has the most number of BPE Tokens?\nChoices:\n(A) Legal \n(B) Code \n(C) Conversational \n(D) Science", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/790.jpg'}", "output": "d"}
{"index": "791", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: In how many years, is the merchandise exports greater than 0.92 %?\nChoices:\n(A) 0\n(B) 1\n(C) 2\n(D) 4", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/791.jpg'}", "output": "b"}
{"index": "792", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Choose the answer for the missing picture.\nChoices:\n(A) A\n(B) B\n(C) C\n(D) D", "answer": "D", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/792.jpg'}", "output": ""}
{"index": "793", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: In how many years, is the percentage of amount earned from merchandise imports in Canada greater than the average percentage of amount earned from merchandise imports in Canada taken over all years ?\nChoices:\n(A) 0\n(B) 2\n(C) 1\n(D) 3", "answer": "B", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/793.jpg'}", "output": "c"}
{"index": "794", "question": "What will happen next?\nOptions: A: the person is gonna fall off the ladder, B: the person is gonna stand still on the ladder, C: someone is gonna come and hold the ladder, D: both A,B, and C", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/794.jpg'}", "output": ""}
{"index": "795", "question": "What will happen next?\nOptions: A: the kid is gonna slide through, B: the kid is gonna crash into the other kid, C: the other kid is gonna dodge, D: both A,B, and C", "answer": "D", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/795.jpg'}", "output": ""}
{"index": "796", "question": "What will happen next?\nOptions: A: the man is gonna walk back, B: the man is gonna fall, C: the man is gonna get up, D: both A,B, and C", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/796.jpg'}", "output": "b"}
{"index": "797", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: The movie critic liked to count the number of actors in each movie he saw. How many movies had at least 30 actors but fewer than 47 actors? (Unit: movies)\nChoices:\n(A) 5\n(B) 3\n(C) 2\n(D) 4", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/797.jpg'}", "output": "b"}
{"index": "798", "question": "Based on the player's body position, what is the most likely outcome of the hit?\nOptions: A: A home run, B: A foul ball, C: A ground ball, D: A fly ball", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/798.jpg'}", "output": ""}
{"index": "799", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Complete the matrix.\nChoices:\n(A) A\n(B) B\n(C) C\n(D) D", "answer": "D", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/799.jpg'}", "output": "d"}
{"index": "800", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Determine the next shape.\nChoices:\n(A) A\n(B) B\n(C) C\n(D) D", "answer": "D", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/800.jpg'}", "output": "d"}
{"index": "801", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Which cube is identical to the unfolded net?\nChoices:\n(A) A\n(B) B\n(C) C\n(D) D", "answer": "D", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/801.jpg'}", "output": ""}
{"index": "802", "question": "Which solution has a higher concentration of green particles?\nOptions: A: neither; their concentrations are the same, B: Solution A, C: Solution B, D: nan", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/802.jpg'}", "output": "c"}
{"index": "803", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Does Yellow Green have the maximum area under the curve?\nChoices:\n(A) yes\n(B) no\n(C) nan\n(D) nan", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/803.jpg'}", "output": "b"}
{"index": "804", "question": "What lies between G and C?\nOptions: A: B, B: none of the above, C: E, D: I", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/804.jpg'}", "output": "c"}
{"index": "805", "question": "What frog shows a warning color?\nOptions: A: O, B: J, C: Q, D: D", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/805.jpg'}", "output": "d"}
{"index": "806", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: In which period the number of full time employees is the maximum?\nChoices:\n(A) Jul '21\n(B) Jun '21\n(C) Mar '21\n(D) May '21", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/806.jpg'}", "output": "d"}
{"index": "807", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: How many people like the most preferred object in the whole chart?\nChoices:\n(A) 4\n(B) 5\n(C) 6\n(D) 9", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/807.jpg'}", "output": "d"}
{"index": "808", "question": "which of the label show maximum leaf\nOptions: A: j, B: k, C: h, D: g", "answer": "B", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/808.jpg'}", "output": "b"}
{"index": "809", "question": "Which of these is spherical?\nOptions: A: B, B: none of the above, C: A, D: D", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/809.jpg'}", "output": ""}
{"index": "810", "question": "What is the main activity of the people on the beach?\nOptions: A: Sunbathing, B: Having a picnic, C: Playing volleyball, D: Swimming", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/810.jpg'}", "output": "d"}
{"index": "811", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Does Dark Violet have the minimum area under the curve?\nChoices:\n(A) yes\n(B) no\n(C) nan\n(D) nan", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/811.jpg'}", "output": ""}
{"index": "812", "question": "Is there a teddy bear in the image?\nOptions: A: Can't tell, B: No, C: Yes, D: Maybe", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/812.jpg'}", "output": "b"}
{"index": "813", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the value of the smallest bar?\nChoices:\n(A) 3\n(B) 1\n(C) 2\n(D) 4", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/813.jpg'}", "output": ""}
{"index": "814", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Is Sky Blue the minimum?\nChoices:\n(A) yes\n(B) no\n(C) nan\n(D) nan", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/814.jpg'}", "output": ""}
{"index": "815", "question": "What is the primary food source for the animals in this image?\nOptions: A: Grains, B: Hay, C: Fruits, D: Vegetables", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/815.jpg'}", "output": "b"}
{"index": "816", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Which of the cubes is the same as the unfolded cube?\nChoices:\n(A) A\n(B) B\n(C) C\n(D) D", "answer": "A", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/816.jpg'}", "output": ""}
{"index": "817", "question": "What is correct Python code to generate the content of the image?\nOptions: A: fruits = [\"apple\", \"banana\", \"cherry\"]\\nfor x in fruits:\\n  print(x), B: mystr = \"banana\"\\nmyit = iter(mystr)\\n\\nprint(next(myit))\\nprint(next(myit))\\nprint(next(myit))\\nprint(next(myit))\\nprint(next(myit))\\nprint(next(myit)), C: i = 1\\nwhile i < 6:\\n  print(i)\\n  i += 1\\n, D: x = lambda a, b: a * b\\nprint(x(5, 6))\\n", "answer": "B", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/817.jpg'}", "output": ""}
{"index": "818", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the value of the smallest bar?\nChoices:\n(A) 0\n(B) 1\n(C) 10\n(D) 5", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/818.jpg'}", "output": "b"}
{"index": "819", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Can you find the missing term?\nChoices:\n(A) 0\n(B) 5\n(C) 1\n(D) 10", "answer": "D", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/819.jpg'}", "output": "c"}
{"index": "820", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the difference between the highest and the lowest value of blue bar?\nChoices:\n(A) 64\n(B) 56\n(C) 66\n(D) 57", "answer": "A", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/820.jpg'}", "output": ""}
{"index": "821", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Use the graph to answer the question below. Which month is the wettest on average in Christchurch?\nChoices:\n(A) August\n(B) April\n(C) May\n(D) March", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/821.jpg'}", "output": "c"}
{"index": "822", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Mr. Huffman, a P.E. teacher, wrote down how much weight each of his students could lift. How many people lifted at least 46 pounds? (Unit: people)\nChoices:\n(A) 0\n(B) 2\n(C) 1\n(D) 3", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/822.jpg'}", "output": "d"}
{"index": "823", "question": "what code would generate this webpage in the browser?\nOptions: A: <!DOCTYPE html>\n<html>\n<body>\n\n<h2>JavaScript Objects</h2>\n\n<p id=\"demo\"></p>\n\n<script>\n// Create an object:\nconst person = {\nfirstName: \"John\",\nlastName: \"Doe\",\nage: 50,\neyeColor: \"blue\"\n};\n\n// Display some data from the object:\ndocument.getElementById(\"demo\").innerHTML =\nperson.firstName + \" is \" + person.age + \" years old.\";\n</script>\n\n</body>\n</html>, B: <!DOCTYPE html>\n<html>\n<body>\n\n<h2>JavaScript Objects</h2>\n\n<p>There are two different ways to access an object property.</p>\n\n<p>You can use person.property or person[\"property\"].</p>\n\n<p id=\"demo\"></p>\n\n<script>\n// Create an object:\nconst person = {\nfirstName: \"John\",\nlastName : \"Doe\",\nid : 5566\n};\n\n// Display some data from the object:\ndocument.getElementById(\"demo\").innerHTML =\nperson[\"firstName\"] + \" \" + person[\"lastName\"];\n</script>\n\n</body>\n</html>, C: <!DOCTYPE html>\n<html>\n<body>\n\n<h2>JavaScript Objects</h2>\n<p>An object method is a function definition, stored as a property value.</p>\n\n<p id=\"demo\"></p>\n\n<script>\n// Create an object:\nconst person = {\nfirstName: \"John\",\nlastName: \"Doe\",\nid: 5566,\nfullName: function() {\nreturn this.firstName + \" \" + this.lastName;\n}\n};\n\n// Display data from the object:\ndocument.getElementById(\"demo\").innerHTML = person.fullName();\n</script>\n\n</body>\n</html>, D: <!DOCTYPE html>\n<html>\n<body>\n\n<h2>JavaScript Objects</h2>\n\n<p>If you access an object method without (), it will return the function definition:</p>\n\n<p id=\"demo\"></p>\n\n<script>\n// Create an object:\nconst person = {\nfirstName: \"John\",\nlastName : \"Doe\",\nid : 5566,\nfullName : function() {\nreturn this.firstName + \" \" + this.lastName;\n}\n};\n\n// Display data from the object:\ndocument.getElementById(\"demo\").innerHTML = person.fullName;\n</script>\n\n</body>\n</html>", "answer": "A", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/823.jpg'}", "output": ""}
{"index": "824", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: How many objects are preferred by more than 7 people in at least one category?\nChoices:\n(A) 0\n(B) 1\n(C) 3\n(D) 4", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/824.jpg'}", "output": "c"}
{"index": "825", "question": "Which object is located closest to the bottom of the image?\nOptions: A: Red carpet, B: Black shoes, C: White sign, D: White banner", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/825.jpg'}", "output": "b"}
{"index": "826", "question": "What is the common reaction of the crowd towards the soccer match?\nOptions: A: Boredom, B: Confusion, C: Disappointment, D: Excitement", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/826.jpg'}", "output": "d"}
{"index": "827", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the accuracy of the algorithm with highest accuracy?\nChoices:\n(A) 4.5\n(B) 5\n(C) 8\n(D) 7.5", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/827.jpg'}", "output": "c"}
{"index": "828", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: Which matchstick needs to be moved in order to create a square?\nChoices:\n(A) Top\n(B) Bottom\n(C) Left\n(D) Right", "answer": "C", "category": "logical reasoning", "l2_category": "code & sequence reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/828.jpg'}", "output": ""}
{"index": "829", "question": "which part of the diagram is large circle?\nOptions: A: C, B: A, C: B, D: D", "answer": "C", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/829.jpg'}", "output": "b"}
{"index": "830", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/830.jpg'}", "output": ""}
{"index": "831", "question": "What will happen next?\nOptions: A: the motorcyle is gonna go forward, B: the motorcyle is gonna crash, C: the motorcyle is gonna go backward, D: both A,B, and C", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/831.jpg'}", "output": "b"}
{"index": "832", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/832.jpg'}", "output": ""}
{"index": "833", "question": "Is the window in front of the woman open or closed?\nOptions: A: Half-open, B: Open, C: Closed, D: Unable to determine", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/833.jpg'}", "output": "c"}
{"index": "834", "question": "What is the referee doing in the image?\nOptions: A: Blowing the whistle, B: Talking to the players, C: Watching the match from the sideline, D: Scoring the goal", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/834.jpg'}", "output": "c"}
{"index": "835", "question": "In which direction is the sun shining in the image?\nOptions: A: North, B: West, C: East, D: South", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/835.jpg'}", "output": "c"}
{"index": "836", "question": "What can be inferred from the position of the desk in the room?\nOptions: A: It's in the center of the room., B: It's near the door., C: It's next to the window., D: It's close to the foot of the bed.", "answer": "D", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/836.jpg'}", "output": "c"}
{"index": "837", "question": "Which piece of clothing has a stripe on it?\nOptions: A: Shirt, B: Sock, C: Pant, D: Shoe", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/837.jpg'}", "output": ""}
{"index": "838", "question": "Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What is the difference between the largest and the smallest value in the chart?\nChoices:\n(A) 75\n(B) 55\n(C) 65\n(D) 70", "answer": "D", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'MathVista', 'split': 'val', 'image_path': 'images/838.jpg'}", "output": "d"}
{"index": "839", "question": "What is the transformation in this image?\nOptions: A: The water will freeze, B: The water will remain liquid, C: The water will evaporate, D: The water will condense", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/839.jpg'}", "output": "d"}
{"index": "840", "question": "Which bird has the longest beak?\nOptions: A: Bird B, B: Bird C, C: Bird A, D: Bird D", "answer": "B", "category": "logical reasoning", "l2_category": "diagram reasoning", "meta_info": "{'source': 'AI2D_TEST', 'split': 'val', 'image_path': 'images/840.jpg'}", "output": ""}
{"index": "841", "question": "What will happen next?\nOptions: A: the bike is gonna get stuck in the mud, B: the bike is gonna run forward, C: the bike is gonna go backwards, D: both A,B, and C", "answer": "A", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/841.jpg'}", "output": "d"}
{"index": "842", "question": "The object shown in this figure:\nOptions: A: Is a metallic element that is essential for life and commonly used in construction and manufacturing, B: Has a relatively low melting point of around 1,538C, C: Is the most abundant element by mass in Earth's core, D: All of these options are correct.", "answer": "D", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/842.jpg'}", "output": "d"}
{"index": "843", "question": "In nature, what's the relationship between these two creatures?\nOptions: A: Predatory relationships, B: Competitive relationships, C: Parasitic relationships, D: Symbiotic relationship", "answer": "B", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'MMBench', 'split': 'val', 'image_path': 'images/843.jpg'}", "output": "b"}
{"index": "844", "question": "Which of the following objects has the highest density in the image?\nOptions: A: Palm trees, B: Trees, C: Sand, D: Dead grass", "answer": "C", "category": "logical reasoning", "l2_category": "common reasoning", "meta_info": "{'source': 'SEEDBench_IMG', 'split': 'val', 'image_path': 'images/844.jpg'}", "output": ""}
